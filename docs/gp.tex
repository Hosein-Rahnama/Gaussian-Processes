\documentclass[10pt]{article}

\usepackage[a4paper,total={17cm,25cm}]{geometry}

\usepackage[fleqn]{amsmath}
\usepackage{amssymb}
\usepackage{mathrsfs}
\usepackage{amsthm}
\usepackage{dsfont}
\usepackage[mathcal]{euscript}

\usepackage{enumitem}

\usepackage{algorithm}
\usepackage{algpseudocode}

\usepackage[colorlinks=true, citecolor=blue]{hyperref}
\usepackage[capitalise]{cleveref}

\usepackage{graphicx}
\usepackage[skip=0cm, font=footnotesize, labelfont=bf, labelsep=period]{caption}
\usepackage[dvipsnames]{xcolor}

\usepackage[backend=biber, style=ieee]{biblatex}
\addbibresource{Refs.bib}
\renewcommand*{\bibfont}{\small}

\newtheorem{thm}{Theorem}[section]
\newtheorem{lem}[thm]{Lemma}
\newtheorem{cor}[thm]{Corollary}
\theoremstyle{definition}
\newtheorem{defn}{Definition}[section]
\newtheorem{exmp}{Example}[section]
\newtheorem*{rem}{Remark}

\crefname{lem}{Lemma}{Lemmas}
\crefname{thm}{Theorem}{Theorems}

\newenvironment{prf}{\noindent\textbf{Proof.}}{\hfill$\blacksquare$}

\counterwithin{equation}{section}
\newcommand\eqnum{\addtocounter{equation}{1}\tag{\arabic{section}.\arabic{equation}}}
\newcommand{\overbar}[1]{\mkern 1.5mu\overline{\mkern-1.5mu#1\mkern-1.5mu}\mkern 1.5mu}
\algnewcommand{\LineComment}[1]{\State {\color{ForestGreen} \(\triangleright\) #1}}

\DeclareMathOperator{\cov}{\mathbb{CV}}
\DeclareMathOperator{\gp}{GP}
\DeclareMathOperator{\tr}{tr}
\DeclareMathOperator{\df}{d\!}
\DeclareMathOperator*{\argmax}{\arg\max\,}

\begin{document}

\begin{titlepage}
\begin{center}
{\Large \scshape Gaussian Processes} \\
\bigskip
H. Rahnama \\
\verb|hosein.rahnama@outlook.com|
\end{center}
\bigskip
\begin{abstract}
This is an introduction to the basics of Gaussian Processes (GP). Particulary, we are interested in their applications for regression and machine learning. The tutorial is mainly divided in two parts. In the first part, we present a step-by-step explanation of the necessary mathematics to understand GPs. For this purpose, we assume that the reader is familiar with linear algebra, probability, and multivariable calculus. In this part, we touch upon the Cholesky decomposition, the matrix inversion lemma for partitioned matrices, MultiVariate Normal (MVN) Probability Density Functions (PDF) for continuous random vectors, and four main operations assosiated with MVNs, namely marginalization, conditioning,  sampling and linear combination. In the second part, we define what a GP is and learn how to do regression with GPs in the cases of noise-free and noisy data observations. Afterwards, we present a simple algorithm to do regression with GPs, investigate its time complexity and implementation. Then, we study the effect of hyperparameters of a special kernel function, well known as the Radial Basis Function (RBF), on the predictive distribution. This sets the motivation for optimizing the hyperparameters, which we shall address briefly via introducing Cross Validation (CV) and Marginal Likelihood (ML). We also talk about the methods of combining available kernels to construct new ones. Afterwards, we address a real world application of GPs for designing interactive interfaces. The paper concludes with some remarks about the connection of GPs with other models and their capability for scaling. \\[\baselineskip]
\textbf{keywords}. Gaussian Process, Multivariate Normal Distribution, Regression, Kernels, Hyperparameters.
\end{abstract}
\tableofcontents
\end{titlepage}


\section{Mathematical Preliminaries}
In the first part of this tutorial, we express the mathematics that we will employ in the second part. Let us first pay attention to a very useful matrix decomposition.

\subsection{Cholesky Decomposition}
This decomposition for Symmetric Positive Definite (SPD) matrices is in perfect analogy with the square root of a positive real number.
\begin{thm}[Cholesky Decomposition]\label{thm:chol}
Let $\mathsf{A} \in \mathbb{R}^{n,n}$ be a real square matrix. The matrix $\mathsf{A}$ is SPD if and only if there exists a non-singular lower-triangular matrix $\mathsf{L}\in \mathbb{R}^{n,n}$ such that $\mathsf{A}=\mathsf{L} \mathsf{L}^{\top}$. This is called the \textbf{Cholesky decomposition} of $\mathsf{A}$ and $\mathsf{L}$ is called the \textbf{Cholesky factor} of $\mathsf{A}$.
\end{thm}
\begin{rem}
Factoring the diagonal elements of $\mathsf{L}$ in the Cholesky decomposition of $\mathsf{A}$, we arrive at $\mathsf{A}=\bar{\mathsf{L}}\mathsf{D}\bar{\mathsf{L}}^\top$ with $\mathsf{D}$ a digonal matrix with positive entries and $\bar{\mathsf{L}}$ an invertible lower triangular matrix with ones on its diagonal. This form is unique, while the Cholesky decompositon is not. More specifically, if $\mathsf{L}$ is a Cholesky factor of matrix $\mathsf{A}$ so is $-\mathsf{L}$.
\end{rem}
From \cref{thm:chol} and the facts that $\det\mathsf{A}\mathsf{B}=\det\mathsf{A}\det\mathsf{B}$, $\det\mathsf{A}=\det\mathsf{A}^\top$, it follows quickly that
\begin{cor}
If $\mathsf{A} \in \mathbb{R}^{n,n}$  is SPD then its determinant is given by $\det\mathsf{A} = (\det\mathsf{L})^2 $.
\end{cor}
The Cholesky decomposition is numerically stable for solving symmetric positive definite linear systems of equations and is the main method of choice for computation if it is available. Below is an algorithm that remedies the computation of Cholesky decomposition. The \Cref{alg:chol} simply assumes that matrix $\mathsf{A}$ has a Cholesky decomposition and exploits the fact that
\begin{equation*}
a_{jk}  = \sum_{i=1}^{n}l_{ji}l_{ki} = 
\sum_{i=1}^{\min\{j, k\}}l_{ji}l_{ki} = 
\sum_{i=1}^{k}l_{ji}l_{ki}, 
\qquad k=1,\dots, n, \qquad j = k,\dots, n
\eqnum
\label{eq:chol}
\end{equation*}
to calculate the columns of $\mathsf{L}$ inductively. When $k=1$, we observe that 
\begin{equation*}
a_{j1} = \sum_{i=1}^{1}l_{ji}l_{1i} = l_{j1}l_{11}, \qquad j = 1,\dots, n
\eqnum
\end{equation*}
Now, considering the cases $j=1$ and $j>1$, one concludes that
\begin{align*}
l_{11} &= \sqrt{a_{11} } \\
l_{j1} &= a_{j1}/l_{11}, \qquad j=2,\dots,n,
\eqnum
\end{align*}
meaning that the first column of $\mathsf{L}$ is computed. Suppose that we have computed the first $k-1$ columns of $\mathsf{L}$ and we want to compute the $k$th column. Again, by considering the cases $j=k$ and $j>k$ in \cref{eq:chol}, we obtain the following inductive equations
\begin{alignat*}{2}
l_{kk} &= \Big(a_{kk} - \sum_{i=1}^{k-1}l_{ki}^2\Big)^{\frac{1}{2}}, && k=2,\dots,n, \\ 
l_{jk} &= \Big(a_{jk}-\sum_{i=1}^{k-1}l_{ki}l_{ji}\Big)/l_{kk}, \qquad && j = k+1,\dots, n,
\eqnum
\label{eq:chol:alg}
\end{alignat*}
where all of the terms on the right hand sides (RHS) are known according to the hypothesis of induction. Using \Cref{alg:chol}, it can be easily verified that
\begin{cor}
Time complexity of computing the Cholesky decomposition of a SPD matrix $A$ is of order $\mathcal{O}(\frac{1}{6}n^3)$.
\end{cor}
\noindent
and we also have the following result
\begin{cor}
Given the Cholesky decomposition, solving the SPD linear system $\mathsf{A}\mathbf{x}=\mathbf{b}$ reduces to solving the lower-triangular $\mathsf{L}\mathbf{y} = \mathbf{b}$ and upper-triangular $\mathsf{L}^{\top} \mathbf{x} = \mathbf{y}$  linear systems. Time complexity of solving each of these is of order $\mathcal{O}(\frac{1}{2}n^2)$.
\end{cor}

\begin{algorithm}[t]
\caption{Cholesky Decomposition}\label{alg:chol}
\hspace*{\algorithmicindent} {\scshape Input}: $\mathsf{A}$ \\
\hspace*{\algorithmicindent} {\scshape Output}: $\mathsf{L}$ \\
\hspace*{\algorithmicindent} {\scshape Cholesky-Decomposition}($\mathsf{A}$):
\begin{algorithmic}[1]
\LineComment{Initialize $\mathsf{L}$ with zeros.}
\For{$j=1$ \textbf{to} $n$}
    \For{$k=1$ \textbf{to} $n$}
	   \State $l_{jk} = 0$
      \EndFor
\EndFor
\LineComment{Compute columns of $\mathsf{L}$ inductively.}
\For{$k=1$ \textbf{to} $n$}
    \LineComment{Calculate $l_{kk}$.}
   	\State sum = $0$
   	\For{$i=1$ \textbf{to} $k-1$}
   		\State sum = sum + $l_{ki}l_{ki}$
   	\EndFor
   	\State $l_{kk} = a_{kk}- \text{sum}$
   	\LineComment{Check for positive definitness.}
   	\If{$l_{kk} \leq 0$}
   		\State $\mathsf{A}$ is not positive definite
   		\State \Return
    \EndIf
   	\State $l_{kk} = \sqrt{l_{kk}}$;
    \LineComment{Calculate $l_{jk}$ for $j>k$.}
    \For{$j=k+1$ \textbf{to} $n$}
       	\State sum = $0$
       	\For{$i=1$ \textbf{to} $k-1$}
       		\State sum = sum + $l_{ki}l_{ji}$
       	\EndFor
       	\State $l_{jk}=(a_{jk}-\text{sum})/l_{kk}$
    \EndFor
\EndFor
\State \Return $\mathsf{L}$
\end{algorithmic}
\end{algorithm}

\subsection{Matrix Inversion Lemma}
Here, we shall discuss an important lemma for inverting a partioned matrix and its imediate consequences. It will be useful in dealing with MVN PDFs. We have the following theorem.
\begin{lem}[Matrix Inversion]\label{lem:MIL}
Consider a general invertible partioned matrix $\mathsf{M}\in\mathbb{R}^{n+m,n+m}$ as
\begin{equation*}
\mathsf{M} =
\begin{bmatrix}
\mathsf{A} & \mathsf{U} \\
\mathsf{V} & \mathsf{B}
\end{bmatrix},
\eqnum
\end{equation*}
where $\mathsf{A}\in\mathbb{R}^{n,n}$ and $\mathsf{B}\in\mathbb{R}^{m,m}$ are invertible. Furthermore, $\mathsf{U}\in\mathbb{R}^{n,m}$ and $\mathsf{V}\in\mathbb{R}^{m,n}$ are $n \times m$ and $m\times n$ matrices, respectively. Then, $\mathsf{M}^{-1}$ is given by
\begin{equation*}
\mathsf{M}^{-1} =
\begin{bmatrix}
(\mathsf{M}/\mathsf{B})^{-1} & 
-(\mathsf{M}/\mathsf{B})^{-1}\mathsf{U}\mathsf{B}^{-1} \\
-\mathsf{B}^{-1}\mathsf{V}(\mathsf{M}/\mathsf{B})^{-1} & 
\mathsf{B}^{-1}+\mathsf{B}^{-1}\mathsf{V}(\mathsf{M}/\mathsf{B})^{-1}\mathsf{U}\mathsf{B}^{-1}
\end{bmatrix},
\eqnum
\label{eq:schur:MB}
\end{equation*}
where $\mathsf{M}/\mathsf{B}:=\mathsf{A}-\mathsf{U}\mathsf{B}^{-1}\mathsf{V}$ is the \textbf{schur complement} of $\mathsf{M}$ with respect to $\mathsf{B}$. We also have
\begin{equation*}
\mathsf{M}^{-1} =
\begin{bmatrix}
\mathsf{A}^{-1}+\mathsf{A}^{-1}\mathsf{U}(\mathsf{M}/\mathsf{A})^{-1}\mathsf{V}\mathsf{A}^{-1} & 
-\mathsf{A}^{-1}\mathsf{U} (\mathsf{M}/\mathsf{A})^{-1} \\
-(\mathsf{M}/\mathsf{A})^{-1}\mathsf{V}\mathsf{A}^{-1} & 
(\mathsf{M}/\mathsf{A})^{-1} 
\end{bmatrix},
\eqnum
\label{eq:schur:MA}
\end{equation*}
where $\mathsf{M}/\mathsf{A}:=\mathsf{B}-\mathsf{V}\mathsf{A}^{-1}\mathsf{U}$ is the \textbf{schur complement} of $\mathsf{M}$ with respect to $\mathsf{A}$. Furthermore, we have that
\begin{equation*}
\det\mathsf{M} = \det\mathsf{B}\det(\mathsf{M}/\mathsf{B})= \det\mathsf{A}\det(\mathsf{M}/\mathsf{A})
\eqnum
\label{eq:schur:det}
\end{equation*}
\end{lem}
\begin{prf}
The idea is pretty simple. We will carry out a block diagonal Gauss-Jordan elimination on $\mathsf{M}$ and everything follows so naturally. Let us make the bottom left block zero by a block row operation as below
\begin{equation*}
\mathsf{R}\mathsf{M} =
\begin{bmatrix}
\mathsf{I_n} & \mathsf{0} \\
-\mathsf{V}\mathsf{A}^{-1} & \mathsf{I_m}
\end{bmatrix}
\begin{bmatrix}
\mathsf{A} & \mathsf{U} \\
\mathsf{V} & \mathsf{B}
\end{bmatrix} =
\begin{bmatrix}
\mathsf{A} & \mathsf{U} \\
\mathsf{0} & \mathsf{B}-\mathsf{V}\mathsf{A}^{-1}\mathsf{U}
\end{bmatrix} =
\begin{bmatrix}
\mathsf{A} & \mathsf{U} \\
\mathsf{0} & \mathsf{M}/\mathsf{A}
\end{bmatrix}.
\eqnum
\label{eq:inv:RM}
\end{equation*}
Next, we shall make the top right block of $\mathsf{R}\mathsf{M}$ zero by a column operation
\begin{equation*}
\mathsf{R}\mathsf{M}\mathsf{C} =
\begin{bmatrix}
\mathsf{A} & \mathsf{U} \\
\mathsf{0} & \mathsf{M}/\mathsf{A}
\end{bmatrix}
\begin{bmatrix}
\mathsf{I_n} & -\mathsf{A}^{-1}\mathsf{U} \\
\mathsf{0} & \mathsf{I_m}
\end{bmatrix} = 
\begin{bmatrix}
\mathsf{A} & \mathsf{0} \\
\mathsf{0} & \mathsf{M}/\mathsf{A}
\end{bmatrix} := \mathsf{D}
\eqnum
\label{eq:D:RMC}
\end{equation*}
Now, we note that $\mathsf{M} = \mathsf{R}^{-1}\mathsf{D}\mathsf{C}^{-1}$, so the inverse can be calculated as $\mathsf{M}^{-1} = \mathsf{C}\mathsf{D}^{-1}\mathsf{R}$. This conclusion is valid since $\mathsf{R}$, $\mathsf{C}$ and $\mathsf{D}$ are invertible. More specifically, notice that
\begin{equation}
\det\mathsf{R} = \det\mathsf{C} = \det\mathsf{I_n}\det\mathsf{I}_m = 1,
\eqnum
\label{eq:det:RC}
\end{equation}
which implies invertibility of $\mathsf{R}$ and $\mathsf{C}$. Moreover, matrix $\mathsf{D}$ is invertible since by \cref{eq:D:RMC} it is the product of three invertible matrices. Futhremore, invertiblity of $\mathsf{D}$ is equivalent to invertibility of $\mathsf{A}$ and $\mathsf{M}/\mathsf{A}$. Finally, we have
\begin{align*}
\mathsf{M}^{-1} &= \mathsf{C}\mathsf{D}^{-1}\mathsf{R} \\
&= 
\begin{bmatrix}
\mathsf{I_n} & -\mathsf{A}^{-1}\mathsf{U} \\
\mathsf{0} & \mathsf{I_m}
\end{bmatrix}
\begin{bmatrix}
\mathsf{A} & \mathsf{0} \\
\mathsf{0} & \mathsf{M}/\mathsf{A}
\end{bmatrix}^{-1}
\begin{bmatrix}
\mathsf{I_n} & \mathsf{0} \\
-\mathsf{V}\mathsf{A}^{-1} & \mathsf{I_m}
\end{bmatrix} \\ &=
\begin{bmatrix}
\mathsf{I_n} & -\mathsf{A}^{-1}\mathsf{U} \\
\mathsf{0} & \mathsf{I_m}
\end{bmatrix}
\begin{bmatrix}
\mathsf{A}^{-1} & \mathsf{0} \\
\mathsf{0} & (\mathsf{M}/\mathsf{A})^{-1}
\end{bmatrix}
\begin{bmatrix}
\mathsf{I_n} & \mathsf{0} \\
-\mathsf{V}\mathsf{A}^{-1} & \mathsf{I_m}
\end{bmatrix} \\ &=
\begin{bmatrix}
\mathsf{A}^{-1} & -\mathsf{A}^{-1}\mathsf{U}(\mathsf{M}/\mathsf{A})^{-1} \\
\mathsf{0} & (\mathsf{M}/\mathsf{A})^{-1}
\end{bmatrix}
\begin{bmatrix}
\mathsf{I_n} & \mathsf{0} \\
-\mathsf{V}\mathsf{A}^{-1} & \mathsf{I_m}
\end{bmatrix} \\ &=
\begin{bmatrix}
\mathsf{A}^{-1} + \mathsf{A}^{-1}\mathsf{U}(\mathsf{M}/\mathsf{A})^{-1}\mathsf{V}\mathsf{A}^{-1} & 
-\mathsf{A}^{-1}\mathsf{U}(\mathsf{M}/\mathsf{A})^{-1} \\
-(\mathsf{M}/\mathsf{A})^{-1}\mathsf{V}\mathsf{A}^{-1} & (\mathsf{M}/\mathsf{A})^{-1}
\end{bmatrix},
\end{align*}
which proves \cref{eq:schur:MA}. Furthermore, combining Eqs. \eqref{eq:D:RMC} and \eqref{eq:det:RC} we obtain
\begin{equation*}
\det\mathsf{M} = \det\mathsf{R}^{-1}\mathsf{D}\mathsf{C}^{-1} = \det\mathsf{R}^{-1} \det\mathsf{D} \det\mathsf{C}^{-1}
= \det\mathsf{D} = \det\mathsf{A}\det(\mathsf{M}/\mathsf{A}),
\end{equation*}
which proves the seond equality in \cref{eq:schur:det}. The proof for the remaining part of the theorem is very similar and we leave it as an exercise. Indeed, it is proved by first making the bottom left block of $\mathsf{M}$ zero by a column operation $\overbar{\mathsf{C}}$ and then making the top right block of $\mathsf{M}\overbar{\mathsf{C}}$ zero by a row operation $\overbar{\mathsf{R}}$.
\end{prf}
\begin{exmp}
An application of the above thoerem is to reduce the computational cost in certain circumtances. Consider the equality of the upper left terms in Eqs. \eqref{eq:schur:MA} and \eqref{eq:schur:MB}
\begin{equation*}
(\mathsf{M}/\mathsf{B})^{-1} = \mathsf{A}^{-1} + \mathsf{A}^{-1}\mathsf{U}(\mathsf{M}/\mathsf{A})^{-1}\mathsf{V}\mathsf{A}^{-1},
\eqnum
\end{equation*}
which is equivalent to
\begin{equation*}
(\mathsf{A} - \mathsf{U}\mathsf{B}^{-1}\mathsf{V})^{-1} = \mathsf{A}^{-1} + \mathsf{A}^{-1}\mathsf{U}(\mathsf{B}-\mathsf{V}\mathsf{A}^{-1}\mathsf{U})^{-1}\mathsf{V}\mathsf{A}^{-1}.
\eqnum
\label{eq:SMW:formula}
\end{equation*}
This is well known as the \textbf{Sherman-Morrison-Woodbury} formula. Next, let $\mathsf{A} = \mathsf{\Sigma}$ be a diagonal matrix, $\mathsf{U} = \mathsf{V}^{\top} = \mathsf{X}$ and $\mathsf{B} = -\mathsf{I_m}$ to obtain
\begin{equation*}
(\mathsf{\Sigma} + \mathsf{X}\mathsf{X}^{\top})^{-1} =
\mathsf{\Sigma}^{-1} - \mathsf{\Sigma}^{-1}\mathsf{X}(\mathsf{I}_m+\mathsf{X}^{\top}\mathsf{\Sigma}^{-1}\mathsf{X})^{-1}\mathsf{X}^{\top}\mathsf{\Sigma}^{-1},
\eqnum
\end{equation*}
where the left hand side (LHS) requires the inversion of an $n\times n$ matrix, while the RHS requires the inversion of an $m\times m$ matrix. When $n \gg m$, it is desirable to use the RHS formulation for calculations instead of the LHS. 
\end{exmp}

\subsection{Univariate Gaussian Distribution}
One of most widely used distributions in statistics and machine learning is the Gaussian or normal distribution. In this subsection, we address how one can sample from a univariate Gaussian distribution.
\begin{defn}
A continuous random variable $X$ is said to be a \textbf{normal} random variable, if its PDF is
\begin{equation*}
p_{X}(x) = \mathcal{N}(x; \mu, \sigma^2) := \frac{1}{\sqrt{2\pi\sigma^2}}\exp\Big(-\frac{(x-\mu)^2}{2\sigma^2}\Big).
\eqnum
\end{equation*}
The distribution is determined by the constants $\mu$ and $\sigma$. 
\end{defn}
\begin{rem}
We write $X\sim \mathcal{N}(x; \mu, \sigma^2)$ as an alternative notation to say that $X$ is a normal random variable. Indeed, this is used to emphasize that the values $x$ taken by $X$ are \textbf{generated}, \textbf{simulated}, or \textbf{sampled} from a Gaussian PDF denoted by $\mathcal{N}(x; \mu, \sigma^2)$. 
\end{rem}
\begin{thm}
Suppose $X$ is a normal random variable. Its mean is $\mathbb{E}(X) = \mu$ and its variance is $\mathbb{V}(X)=\sigma$. Furthermore, the cumulative probability function (CDF) is given by
\begin{equation*}
F_{X}(x) = \mathbb{P}(X \leq x) = \Phi\Big(\frac{x-\mu}{\sigma}\Big),
\eqnum
\end{equation*}
with $\Phi(x)$ defined as
\begin{equation*}
\Phi(x) := \frac{1}{\sqrt{2\pi}}\int_{-\infty}^{x}\exp\Big(-\frac{1}{2}t^2\Big)\df t.
\eqnum
\end{equation*}
\end{thm}
\begin{rem}
For the special case $\mu = 0$ and $\sigma = 1$, the associated random variable is called \textbf{standard normal} and usually denoted by $Z$. The PDF and CDF of $Z$ are given by
\begin{align*}
p_{Z}(z) &= \mathcal{N}(z; 0, 1) = \frac{1}{\sqrt{2\pi}}\exp\Big(-\frac{1}{2}z^2\Big), \\
F_Z(z) &= \mathbb{P}(Z\leq z) = \int_{-\infty}^{z}p_Z(t)\df t = 
\frac{1}{\sqrt{2\pi}}\int_{-\infty}^{z}\exp\Big(-\frac{1}{2}t^2\Big)\df t = \Phi(z).
\eqnum
\end{align*}
\end{rem}
The following theorem is crucial when we want to obtain the PDF of a random variable which is given as a function of another random variable, whose PDF is known. A proof of this thoerem is given in \cite{Gharamani2019}.
\begin{thm}[Method of Transformations]\label{thm:MT}
Let $X$ be a continuous random variable, $p_X$ be the PDF of $X$, and $A\subseteq\mathbb{R}$ be the set of possible values of $X$. For the invertible function $h:A\to\mathbb{R}$, let $Y=h(X)$ be a random variable with the set of possible values $B=h(A)=\{h(a): a\in A\}$. Suppose that $h^{-1}$ is differentiable for all values of $y\in B$. Then, the PDF of $Y$  is given by
\begin{equation*}
p_Y(y) = 
\begin{cases}
p_X(h^{-1}(y))\left|(h^{-1})'(y)\right| & \forall y \in B  \\
0 & \forall y \notin B 
\end{cases}
\eqnum
\end{equation*}
\end{thm}
\noindent
An imediate and useful result of \cref{thm:MT} is the following lemma, which makes a connection between normal and standard normal random variables.
\begin{lem}
Assume that $X=\sigma Z + \mu$ holds for two random variables $X$ and $Z$. Then, $X\sim\mathcal{N}(x; \mu, \sigma^2)$ is a normal random variable if and only if $Z\sim\mathcal{N}(z; 0, 1)$  is a standard normal random variable.
\end{lem}
\begin{prf}
Assume that $Z$ is a standard normal random variable. Take $X=h(Z)=\sigma Z + \mu$. Consequently, we have $A=B=\mathbb{R}$ and $h^{-1}(x)=(x-\mu)/\sigma$. Applying the method of transformations, we obtain
\begin{equation*}
p_X(x)=|(h^{-1})'(x)|p_Z(h^{-1}(x))
=\frac{1}{\sigma}\frac{1}{\sqrt{2\pi}}\exp\Big(-\frac{1}{2}\Big(\frac{x-\mu}{\sigma}\Big)^2\Big)
=\frac{1}{\sqrt{2\pi\sigma^2}}\exp\Big(-\frac{(x-\mu)^2}{2\sigma^2}\Big),
\end{equation*}
which verfies that  $X\sim\mathcal{N}(x; \mu, \sigma^2)$. The proof for the other direction is completely similar.
\end{prf}
\begin{defn}
A random variable $U$ is said to be \textbf{uniform} on the interval $(a, b)$ if its PDF is given by
\begin{equation*}
p_U(x) = \mathcal{U}(x; a, b) :=
\begin{cases} 
\frac{1}{b-a} & x \in (a, b) \\
0 & x \notin (a, b)
\end{cases}.
\eqnum
\end{equation*}
The distribution is determined by the constants $a$ and $b$. 
\end{defn}
\begin{thm}
Suppose $U$ is a uniform random variable. Its mean is $\mathbb{E}(U) = \frac{1}{2}(a+b)$ and its variance is $\mathbb{V}(U)=\frac{1}{12}(b-a)^2$. Furthermore, its CDF is given by
\begin{equation*}
F_U(x) = 
\begin{cases} 
0 & x < a \\
\frac{x-a}{b-a} & x \in (a, b) \\
1 & x > b
\end{cases}.
\eqnum
\end{equation*}
\end{thm}
\begin{rem}
For the special case, where $a=0$ and $b=1$, the PDF is simplified as
\begin{equation*}
p_U(x) = \mathcal{U}(x; 0, 1) :=
\begin{cases} 
1 & x \in (0, 1) \\
0 & x \notin (0, 1)
\end{cases},
\eqnum
\end{equation*}
and consequenlty, the CDF becomes
\begin{equation*}
F_U(x) = 
\begin{cases} 
0 & x \leq 0 \\
x & x \in (0, 1) \\
1 & x \ge 1
\end{cases}.
\eqnum
\end{equation*}
\end{rem}
 Computers have a pseudo random number generator which is capable of simulating the sampling from $\mathcal{U}(x; 0, 1)$. To sample from other PDFs, they employ the following important result, which is again a result of the method of tranformations.
\begin{cor}[Sampling]\label{thm:sample}
Let $U\sim\mathcal{U}(u; 0, 1)$ be a uniform random variable  and let $Y=F_Z^{-1}(U)$ be another random variable $Y$, where $F_Z$ is the CDF of an arbitrary random variable $Z$ that we want to sample. If $F_Z$ is differentable on $\mathbb{R}$ and strictly increasing, then $Y$ and $Z$ have the same PDF.
\end{cor}
\begin{prf}
Take $h=F_Z^{-1}$ so that $Y=h(U) = F_Z^{-1}(U)$ with $F_Z:\mathbb{R} \to [0, 1]$ being the CDF of a random variable $Z$ that we want to sample. Also, note that $A=[0, 1]$, $B=\mathbb{R}$ and $F_Z(\mathbb{R})=(0, 1)$ since $F_Z$ is strictly increasing. Applying the method of transformations gives
\begin{equation*}
p_Y(y) = p_U(F_Z(y))\left|F_Z'(y)\right|=1\cdot\left|F_Z'(y)\right|= F_Z'(y)=p_Z(y),  \qquad \forall y \in \mathbb{R},
\eqnum
\end{equation*}
proving that $Y$ and $Z$ have the same PDF.
\end{prf}
\newline
\begin{rem}
This result is crucial and illustrates that if we are capable of sampling from $\mathcal{U}(x; 0, 1)$, then we can sample from any other distribution with a differentiable strictly increasing CDF. In particular, if you set $F_Z(z) = \Phi((z-\mu)/\sigma)$, which satisfies the aforementioned requirements, then you can sample from $Z\sim\mathcal{N}(z; \mu, \sigma^2)$. For this purpose, draw a sample $u$ from $U\sim\mathcal{U}(u; 0, 1)$ and then calculate $z=F_Z^{-1}(u)$. This is how a computer samples a normal random variable.
\end{rem}

\subsection{Multivariate Gaussian Distribution}
The Multivariate Gaussian or Normal (MVN) distribution is the most widely used joint PDF for continuous random vectors. In this subsection, we address four main operations with MVNs, including marginalization, conditioning,  sampling and linear combination.
\begin{defn}
A continuous random vector $\mathbf{X}$ is said to be a \textbf{normal} random vector, if its joint PDF is
\begin{equation*}
p_{\mathbf{X}}(\mathbf{x}) = \mathcal{N}(\mathbf{x};\boldsymbol{\mu},\mathsf{\Sigma}) := 
\frac{1}{\sqrt{\det2\pi\mathsf{\Sigma}}}
\exp\Big(-\frac{1}{2}(\mathbf{x} - \boldsymbol{\mu})^{\top}\mathsf{\Sigma}^{-1}(\mathbf{x} - \boldsymbol{\mu})\Big),
\eqnum
\end{equation*}
where $\mathbf{x}, \boldsymbol{\mu} \in \mathbb{R}^m$ and $\mathsf{\Sigma} \in \mathbb{R}^{m,m}$ is SPD. The distribution is determined by the vector $\boldsymbol{\mu}$ and matrix $\mathsf{\Sigma}$. 
\end{defn}
\begin{thm}
If $\mathbf{X}$ is a normal random vector, then its \textbf{mean vector} is $\mathbb{E}(\mathbf{X})=\boldsymbol{\mu}$. Furthermore, the \textbf{covariance matrix} is $\cov(\mathbf{X}, \mathbf{X})=\mathbb{V}(\mathbf{X})=\mathsf{\Sigma}$.
\end{thm}
\begin{rem}
The inverse of the covariance matrix $\mathsf{\Lambda} = \mathsf{\Sigma}^{-1}$ is called the \textbf{precision} matrix.
\end{rem}
\begin{rem}
When $\boldsymbol{\mu} = \mathbf{0}$ and $\mathsf{\Sigma} = \mathsf{I}$, the associated random vector is denoted by $\mathbf{Z}$ and is called the \textbf{standard normal}  random vector. In this case, the PDF simplifies to
\begin{align*}
p_{\mathbf{Z}}(\mathbf{z}) &=  \mathcal{N}(\mathbf{z};\boldsymbol{0},\mathsf{I}) := \frac{1}{\sqrt{2\pi}^m}
\exp\Big(-\frac{1}{2}\mathbf{z}^{\top}\mathbf{z}\Big)
\eqnum
\end{align*}
\end{rem}
The following is the multivariate version of the method of transformations, which is a result of the change of variables theorem in multiple integrals.
\begin{thm}[Method of Transformations]\label{thm:MT:MV}
Let $\mathbf{X}$ be a continuous random vector, $p_\mathbf{X}$ be the PDF of $\mathbf{X}$ and $A\subseteq\mathbb{R}^m$ be the set of possible values of $\mathbf{X}$. For the invertible function $h:A\to\mathbb{R}^m$, let $\mathbf{Y}=h(\mathbf{X})$ be a random vector with the set of possible values $B=h(A)=\{h(a), a\in A\}$. Suppose that $h^{-1}$ has continuous partial derivatives and nonzero Jacobian at all points $\mathbf{y}\in B$; that is
\begin{equation*}
\det\mathsf{J}(h^{-1})(\mathbf{y})\ne 0, \qquad \forall \mathbf{y}\in B, \qquad \mathsf{J}(h^{-1})_{ij} := \frac{\partial (h^{-1})_i}{\partial y_j}.
\eqnum
\end{equation*}
Then, the PDF of random vector $\mathbf{Y}$ is given by
\begin{equation*}
p_{\mathbf{Y}}(\mathbf{y}) = 
\begin{cases}
p_{\mathbf{X}}(h^{-1}(\mathbf{y})) |\det\mathsf{J}(h^{-1})(\mathbf{y})| & \mathbf{y} \in B \\
0 & \mathbf{y} \notin B
\end{cases}
\cdot
\eqnum
\end{equation*}
\end{thm}
An application of the multivariate version of the method of transformations will be the following lemma, which relates normal and standard normal random vectors.
\begin{lem}
Assume that $\mathbf{X}=\mathsf{L}\mathbf{Z}+\boldsymbol{\mu}$ holds for two random vectors $\mathbf{X}$ and $\mathbf{Z}$, where $\mathsf{L}$ is the cholesky factor of a PSD matrix $\mathsf{\Sigma}$. Then $\mathbf{X}\sim\mathcal{N}(\mathbf{x};\boldsymbol{\mu},\mathsf{\Sigma})$ is a normal random vector if and only if $\mathbf{Z}\sim\mathcal{N}(\mathbf{z};\mathbf{0},\mathsf{I})$ is a standard normal random vector.
\end{lem}
\begin{prf}
Assume that $\mathbf{Z}=h(\mathbf{X})=\mathsf{L}^{-1}(\mathbf{X}-\boldsymbol{\mu})$ and $\mathbf{X}$ is normal. According to \cref{thm:MT:MV}, $A=B=\mathbb{R}^m$, $\mathbf{z}=h(\mathbf{x})=\mathsf{L}^{-1}(\mathbf{x}-\boldsymbol{\mu})$. So the inverse map and its Jacobian are  $\mathbf{x}=h^{-1}(\mathbf{z})=\mathsf{L}\mathbf{z}+\boldsymbol{\mu}$ and $\mathsf{J}(h^{-1})(\mathbf{z}) = \mathsf{L}$. Consequently, for every $\mathbf{x}\in\mathbb{R}^m$ we have
\begin{align*}
p_{\mathbf{Z}}(\mathbf{z}) &= |\det\mathsf{J}(h^{-1})(\mathbf{z})| \,\, p_{\mathbf{X}}(h^{-1}(\mathbf{z}))  = |\det\mathsf{L}|\,\, p_{\mathbf{X}}(\mathsf{L}\mathbf{z}+\boldsymbol{\mu}) \\
&= \frac{|\det\mathsf{L}|}{(2\pi)^{\frac{m}{2}} \sqrt{\det\mathsf{\Sigma}}} \exp \Big(-\frac{1}{2} ((\mathsf{L}\mathbf{z}+\boldsymbol{\mu})-\boldsymbol{\mu}))^{\top}\mathsf{\Sigma}^{-1}((\mathsf{L}\mathbf{z}+\boldsymbol{\mu})-\boldsymbol{\mu}) \Big)\\
&= \frac{|\det\mathsf{L}|}{(2\pi)^{\frac{m}{2}}|\det\mathsf{L}|} \exp \Big(-\frac{1}{2} (\mathbf{z}^{\top}\mathsf{L}^{\top})(\mathsf{L}^{-\top} \mathsf{L}^{-1})(\mathsf{L}\mathbf{z}) \Big)\\
&=\frac{1}{(2\pi)^{\frac{m}{2}}} \exp \Big(-\frac{1}{2} \mathbf{z}^{\top} \mathbf{z} \Big),
\end{align*}
showing that $\mathbf{Z}$ is standard normal. The proof for the other direction is similar.
\end{prf}
\begin{thm}[Marginalization and Conditioning]\label{thm:MVN:MC}
Suppose $\mathbf{Y}\sim\mathcal{N}(\mathcal{\mathbf{y}; \boldsymbol{\mu}, \mathsf{\Sigma}})$ is jointly Gaussian and is partitioned in two parts with dimensions $m$ and $n$, respectively. We have
\begin{equation}
\mathbf{Y} =
\begin{bmatrix}
\mathbf{Y}_1 \\
\mathbf{Y}_2
\end{bmatrix}, \qquad
\boldsymbol{\mu} = 
\begin{bmatrix}
\boldsymbol{\mu_1} \\
\boldsymbol{\mu_2}
\end{bmatrix}, \qquad
\mathsf{\Sigma} = 
\begin{bmatrix}
\mathsf{\Sigma_{11}} & \mathsf{\Sigma_{12}} \\
\mathsf{\Sigma_{21}} & \mathsf{\Sigma_{22}}
\end{bmatrix},
\end{equation}
where $\boldsymbol{\mu}_1\in\mathbb{R}^n$, $\boldsymbol{\mu}_2\in\mathbb{R}^m$, $\mathsf{\Sigma}_{11}\in\mathbb{R}^{n,n}$, $\mathsf{\Sigma}_{22}\in\mathbb{R}^{m,m}$, $\mathsf{\Sigma}_{21} = \mathsf{\Sigma}_{12}^{\top}\in\mathbb{R}^{m,n}$. Then, the marginal PDFs are given by
\begin{align*}
p_{\mathbf{Y}_1}(\mathbf{y}_1) &= \int_{\mathbf{y}_1\in\mathbb{R}^n}p_{\mathbf{Y}_1,\mathbf{Y}_2}(\mathbf{y}_1, \mathbf{y}_2)\df \mathbf{y}_2 = \mathcal{N}(\mathbf{y}_1; \boldsymbol{\mu}_1, \mathsf{\Sigma}_{11}), \\
p_{\mathbf{Y}_2}(\mathbf{y}_2) &= \int_{\mathbf{y}_2\in\mathbb{R}^m}p_{\mathbf{Y}_1,\mathbf{Y}_2}(\mathbf{y}_1, \mathbf{y}_2)\df \mathbf{y}_1 = \mathcal{N}(\mathbf{y}_2; \boldsymbol{\mu}_2, \mathsf{\Sigma}_{22}),
\eqnum
\label{eq:MVN:marg}
\end{align*}
and the conditional PDF is given by
\begin{align*}
p_{\mathbf{Y}_1|\mathbf{Y}_2}(\mathbf{y}_1|\mathbf{y}_2) &= 
\frac{p_{\mathbf{Y}_1,\mathbf{Y}_2}(\mathbf{y}_1,\mathbf{y}_2)}{p_{\mathbf{Y}_2}(\mathbf{y}_2)} = \mathcal{N}(\mathbf{y}_1;\boldsymbol{\mu}_{1|2},\mathsf{\Sigma}_{1|2}), \\
p_{\mathbf{Y}_2|\mathbf{Y}_1}(\mathbf{y}_2|\mathbf{y}_1) &= 
\frac{p_{\mathbf{Y}_2,\mathbf{Y}_1}(\mathbf{y}_2,\mathbf{y}_1)}{p_{\mathbf{Y}_1}(\mathbf{y}_1)} = \mathcal{N}(\mathbf{y}_2;\boldsymbol{\mu}_{2|1},\mathsf{\Sigma}_{2|1}),
\eqnum
\end{align*}
with its mean vector and covariance matrix obtained as
\begin{align*}
\boldsymbol{\mu}_{1|2} &= \boldsymbol{\mu}_1 + \mathsf{\Sigma}_{12}\mathsf{\Sigma}_{22}^{-1}(\mathbf{y}_2-\boldsymbol{\mu}_2), \qquad
\mathsf{\Sigma}_{1|2} = \mathsf{\Sigma}_{11} - \mathsf{\Sigma}_{12} \mathsf{\Sigma}_{22}^{-1} \mathsf{\Sigma}_{21}, \\
\boldsymbol{\mu}_{2|1} &= \boldsymbol{\mu}_2 + \mathsf{\Sigma}_{21}\mathsf{\Sigma}_{11}^{-1}(\mathbf{y}_1-\boldsymbol{\mu}_1), \qquad
\mathsf{\Sigma}_{2|1} = \mathsf{\Sigma}_{22} - \mathsf{\Sigma}_{21} \mathsf{\Sigma}_{11}^{-1} \mathsf{\Sigma}_{12},
\eqnum
\label{eq:MVN:cond}
\end{align*}
showing that Gaussian PDFs are closed under marginalization and conditioning.
\end{thm}
\begin{prf}
The proof of this theorem is lengthy. For a detailed treatment see \cite{Do2009}. However, we mention an outline of the proof. To obtain the relation for marginals or conditionals carry out the following steps
\begin{enumerate}[noitemsep]
\item Write the integral form of the marginal and the algebraic expression of conditional density explicitly.
\item Rewrite the integral and the expression by partitioning the precision matrix $\mathsf{\Lambda}=\mathsf{\Sigma}^{-1}$.
\item Use a completion-of-squares technique to write the exponents in the integral and  expression in terms of quadratic forms for $\mathbf{y}_1$ and $\mathbf{y}_2$ with the corresponding sub-blocks of $\mathsf{\Lambda}$.
\item Use the matrix inversion lemma to determine sub-blocks of the precision matrix $\mathsf{\Lambda}$ in terms of the sub-blocks of the covariance matrix $\mathsf{\Sigma}$ and argue that the resulting density is Gaussian.
\end{enumerate}
This illustrates an outline for the proof.
\end{prf}
\newline
\indent
\cref{thm:MVN:MC} is central in working with MVN distributions. The recap is that if you want the marginals then just take corresponding block in the mean vector and covariance matrix of the joint distribution. To get the conditional, use \cref{eq:MVN:cond}. Note that the mean vector $\boldsymbol{\mu}_{1|2}$ in the conditional is linear in $\mathbf{y}_2$ and the covariance matrix $\mathsf{\Sigma}_{1|2}$ is just a constant matrix independent of $\mathbf{y}_2$. The same is true for the other conditional.
\begin{rem}
For a standard normal random vector $\mathbf{Z}=(Z_1,\dots,Z_m)^{\top}$, the PDF simplifies to
\begin{align*}
p_{\mathbf{Z}}(\mathbf{z}) 
=  \frac{1}{\sqrt{2\pi}^m} \exp\Big(-\frac{1}{2}\mathbf{z}^{\top}\mathbf{z}\Big) 
= \prod_{i=1}^{m}\frac{1}{\sqrt{2\pi}}\exp\Big(-\frac{1}{2}z_i^2\Big) 
= \prod_{i=1}^{m}\mathcal{N}(z_i; 0, 1)=\prod_{i=1}^{m}p_{Z_i}(z_i),
\eqnum
\label{eq:Ran:Vec:SN:PDF}
\end{align*}
showing that in this case the PDF becomes the products of marginal PDFs. The last equality is a result of marginalizatoin property of MVNs stated in \cref{thm:MVN:MC}. Now, we are able to sample a random normal vector $\mathbf{X}\sim\mathcal{N}(\mathbf{x}; \boldsymbol{\mu}, \mathsf{\Sigma})$. For this purpose, we first calculate the Cholesky factor $\mathsf{L}$ of $\mathsf{\Sigma}$. Then, we note that $\mathbf{X} = \mathsf{L} \mathbf{Z} + \boldsymbol{\mu}$, where $\mathbf{Z}\sim\mathcal{N}(\mathbf{z}; \mathbf{0}, \mathsf{I})$. Now, we are able to sample the random strandard normal vector $\mathbf{Z}=(Z_1,\dots,Z_m)^{\top}$ since by the marginalization property of Gaussians we know that $Z_i\sim\mathcal{N}(z_i, 0, 1)$.
\label{exmp:sample}
\end{rem}
\begin{defn}
For a continuous random vector $\mathbf{X}$, define
\begin{equation}
M_{\mathbf{X}}(\mathbf{t})  
:= \mathbb{E}\big(\exp(\mathbf{t}^\top\mathbf{X})\big)
= \int_{\mathbb{R}^m}\exp(\mathbf{t}^{\top}\mathbf{x})\,p_{\mathbf{X}}(\mathbf{x})\df\mathbf{x}.
\end{equation}
If $M_{\mathbf{X}}(\mathbf{t})$ is finite for all $\mathbf{t}$ in a neighborhood around the origin $B_{\mathbb{R}^m}(\mathbf{0}_m, r)$ with radius $r>0$, then $M_{\mathbf{X}}(\mathbf{t})$ is called the \textbf{Moment Generating Function} (MGF) of $\mathbf{X}$. If not, we say that MGF of  $\mathbf{X}$ does not exist.
\end{defn}
\begin{exmp}
Let $\mathbf{Z}\sim\mathcal{N}(\mathbf{z}; \boldsymbol{\mu}, \mathsf{\Sigma})$ be a nomral random vector. We want to calculate its MGF. The procedure is straight forward as below
\begin{align*}
M_{\mathbf{Z}}(\mathbf{t}) = \mathbb{E}(\exp(\mathbf{t}^{\top}\mathbf{Z})) 
&= \int_{\mathbb{R}^m}\exp(\mathbf{t}^{\top}\mathbf{z})\frac{1}{\sqrt{2\pi}^m}\exp\Big(-\frac{1}{2}\mathbf{z}^{\top}\mathbf{z}\Big)\df \mathbf{z} \\
&= \int_{\mathbb{R}^m}\frac{1}{\sqrt{2\pi}^m}\exp\Big(-\frac{1}{2}(\mathbf{z}^{\top}\mathbf{z}-2\mathbf{t}^{\top}\mathbf{z})\Big)\df \mathbf{z} \\
&= \int_{\mathbb{R}^m}\frac{1}{\sqrt{2\pi}^m}\exp\Big(-\frac{1}{2}\big((\mathbf{z}-\mathbf{t})^{\top}(\mathbf{z}-\mathbf{t})-\mathbf{t}^{\top}\mathbf{t}\big)\Big)\df \mathbf{z} \\
&= \exp\Big(\frac{1}{2}\mathbf{t}^{\top}\mathbf{t}\Big)\int_{\mathbb{R}^m}\frac{1}{\sqrt{2\pi}^m}\exp\Big(-\frac{1}{2}\mathbf{u}^{\top}\mathbf{u}\Big)\df \mathbf{u} \\
&= \exp\Big(\frac{1}{2}\mathbf{t}^{\top}\mathbf{t}\Big)\int_{\mathbb{R}^m}p_{\mathbf{Z}}(\mathbf{u})\df \mathbf{u} \\
&= \exp\Big(\frac{1}{2}\mathbf{t}^{\top}\mathbf{t}\Big).
\eqnum
\label{eq:MGF:SN}
\end{align*}
\end{exmp}
Now, we shall mention some lemmas and theorems that enable us to take advantage of the power of MGFs for obtaining the PDF of a wide range of random vectos. Using the following lemma, we can compute the MGF for a normal random vector. 
\begin{lem}
Let $\mathbf{X}$ and $\mathbf{Y}$ be continous random vectors in $\mathbb{R}^n$ and  $\mathbb{R}^m$, respectively. Suppose that $\mathbf{Y} = \mathsf{A} \mathbf{X} + \mathbf{b} $ with $\mathsf{A}\in\mathbb{R}^{m,n}$ and $\mathbf{b}\in\mathbb{R}^m$. Then, their MGFs are related by
\begin{equation*}
M_{\mathbf{Y}}(\mathbf{t}) = \exp(\mathbf{t}^{\top}\mathbf{b}) M_{\mathbf{X}}(\mathsf{A}^{\top}\mathbf{t}).
\eqnum
\label{eq:MGF:lin}
\end{equation*}
\end{lem}
\begin{prf}
It is just a matter of using definitions.
\begin{align*}
M_{\mathbf{Y}}(\mathbf{t}) &= \mathbb{E}(\exp(\mathbf{t}^{\top}\mathbf{Z}))
= \mathbb{E}(\exp(\mathbf{t}^{\top}\mathsf{A}\mathbf{Z}+\mathbf{t}^{\top}\mathbf{b}))
=\exp(\mathbf{t}^{\top}\mathbf{b})\mathbb{E}((\mathsf{A}^{\top}\mathbf{t})^{\top}\mathbf{Z}) \\
&= \exp(\mathbf{t}^{\top}\mathbf{b})M_{\mathbf{X}}(\mathsf{A}^{\top}\mathbf{t}),
\end{align*}
which completes the proof.
\end{prf}
\begin{exmp}
Let $\mathbf{X}\sim\mathcal{N}(\mathbf{x};\boldsymbol{\mu},\mathsf{\Sigma})$ be a nomral random vector. We know that $\mathbf{X} = \mathsf{L}\mathbf{Z}+\boldsymbol{\mu}$, where $\mathsf{L}$ is the Cholesky factor of $\mathsf{\Sigma}$ and $\mathbf{Z}\sim\mathcal{N}(\mathbf{z}, \mathbf{0}, \mathsf{I})$. Using Eqs. \eqref{eq:MGF:SN} and \eqref{eq:MGF:lin}, we have
\begin{align*}
M_{\mathbf{X}}(\mathbf{t}) 
&= \exp(\mathbf{t}^{\top}\boldsymbol{\mu}) M_{\mathbf{Z}}(\mathsf{L}^{\top}\mathbf{t})
= \exp(\mathbf{t}^{\top}\boldsymbol{\mu}) \exp\Big(\frac{1}{2}\mathbf{(\mathsf{L}^{\top}\mathbf{t}})^{\top}(\mathbf{\mathsf{L}^{\top}\mathbf{t}})\Big)
= \exp(\mathbf{t}^{\top}\boldsymbol{\mu}) \exp\Big(\frac{1}{2}\mathbf{t}^{\top}(\mathsf{L}\mathsf{L}^{\top})\mathbf{t}\Big) \\
&= \exp\Big(\frac{1}{2}\mathbf{t}^{\top}\mathsf{\Sigma}\,\mathbf{t}+\mathbf{t}^{\top}\boldsymbol{\mu}\Big).
\eqnum
\label{eq:MGF:N}
\end{align*}
\end{exmp}
We have the following important result. It shows that an MGF determines the PDF uniquely. That is, if two random vectors have the same MGF, then they are identically distributed \cite{Gharamani2019}.
\begin{thm}[Uniqueness of MGF]
Let $\mathbf{X}$ and $\mathbf{Y}$ be two continuous random vectors. Then, $M_{\mathbf{X}}(\mathbf{t})=M_{\mathbf{Y}}(\mathbf{t})$ for all $\mathbf{t}\in B_{\mathbb{R}^m}(\mathbf{0}_m, r)$ with some $r>0$ if and only if  $p_{\mathbf{X}}(\mathbf{z})=p_{\mathbf{Y}}(\mathbf{z})$  for all $\mathbf{z}\in\mathbb{R}^m$.
\end{thm}
The uniqueness theorem along with the following theorem give us a powerful tool for determining the distribution of the sum of independent random vectors.
\begin{thm}[MGF of Sums]\label{thm:MGF:sum}
Let $\mathbf{X}_1,\dots,\mathbf{X}_k$ be independent random vectors and define $\mathbf{X}=\mathbf{X}_1+\dots+\mathbf{X}_k$. Then, MGF of $\mathbf{X}$ is given by
\begin{equation*}
M_{\mathbf{X}}(\mathbf{t}) = M_{\mathbf{X}_1}(\mathbf{t}) \dots M_{\mathbf{X}_k}(\mathbf{t}).
\eqnum
\label{eq:MGF:sum}
\end{equation*}
\end{thm}
\begin{prf}
Again, we stick to the definitions and everything follows so naturally.
\begin{align*}
M_{\mathbf{X}}(\mathbf{t}) &= \mathbb{E}(\exp(\mathbf{t}^{\top}\mathbf{X}))
= \mathbb{E}(\exp(\mathbf{t}^{\top}\mathbf{X}_1+\dots+\mathbf{t}^{\top}\mathbf{X}_k)) \\
&= \mathbb{E}(\exp(\mathbf{t}^{\top}\mathbf{X}_1)\dots\exp(\mathbf{t}^{\top}\mathbf{X}_k))
= \mathbb{E}(\exp(\mathbf{t}^{\top}\mathbf{X}_1))\dots\mathbb{E}(\exp(\mathbf{t}^{\top}\mathbf{X}_k)) \\
&= M_{\mathbf{X}_1}(\mathbf{t}) \dots M_{\mathbf{X}_k}(\mathbf{t}),
\end{align*}
where we made use of the fact that $\exp(\mathbf{t}^{\top}\mathbf{X}_i)$ with $i=1,\dots,k$ are independent random variables as $\mathbf{X}_i$ are independent random vectors. This completes the proof.
\end{prf}
\begin{thm}[Linear Combination of Independent Gaussians]\label{thm:MVN:LC}
Let $\mathbf{X}_i$ with $p_{\mathbf{X}_i}(\mathbf{x})=\mathcal{N}(\mathbf{x}; \boldsymbol{\mu}_i, \mathsf{\Sigma}_i)$  for $i=1,\dots,k$ be independent normal random vectors and define $\mathbf{X}=c_1\mathbf{X}_1+\dots+c_k\mathbf{X}_k$, where $c_i\in\mathbb{R}$. Then, $\mathbf{X}$ is a normal random vector and its PDF is given by
\begin{equation*}
p_{\mathbf{X}}(\mathbf{x}) = \mathcal{N}\Big(\mathbf{x}; \sum_{i=1}^{k}c_i\boldsymbol{\mu}_i, \sum_{i=1}^{k}c_i^2\mathsf{\Sigma}_i\Big).
\eqnum
\label{eq:MVN:LC}
\end{equation*}
\end{thm}
\begin{prf}
Let us first define $\mathbf{Y}_i=c_i\mathbf{X}_i$. According to \cref{eq:MGF:lin}, taking $\mathsf{A}=c_i\mathsf{I}$ and $\mathbf{b}=\mathbf{0}$ leads to $M_{\mathbf{Y}_i}(\mathbf{t})=M_{\mathbf{X}_i}(c_i\mathbf{t})$.  Notice that $\mathbf{X} = \mathbf{Y}_1 + \dots \mathbf{Y}_k$ and use \cref{thm:MGF:sum} along with \cref{eq:MGF:N} to obtain
\begin{align*}
M_{\mathbf{X}}(\mathbf{t}) &= M_{\mathbf{Y}_1}(\mathbf{t}) \dots M_{\mathbf{Y}_k}(\mathbf{t}) 
= M_{\mathbf{X}_1}(c_1\mathbf{t}) \dots M_{\mathbf{X}_k}(c_k\mathbf{t}) \\
&= \exp\Big(\frac{1}{2}(c_1\mathbf{t})^{\top}\mathsf{\Sigma}_1\,(c_1\mathbf{t})+(c_1\mathbf{t})^{\top}\boldsymbol{\mu}_1\Big) \dots
\exp\Big(\frac{1}{2}(c_k\mathbf{t})^{\top}\mathsf{\Sigma}_k\,(c_k\mathbf{t})+(c_k\mathbf{t})^{\top}\boldsymbol{\mu}_k\Big) \\
&= \exp\Big(\frac{1}{2}c_1^2\mathbf{t}^{\top}\mathsf{\Sigma}_1\,\mathbf{t}+c_1\mathbf{t}^{\top}\boldsymbol{\mu}_1\Big) \dots
\exp\Big(\frac{1}{2}c_k^2\mathbf{t}^{\top}\mathsf{\Sigma}_k\,c_k\mathbf{t}+c_k\mathbf{t}^{\top}\boldsymbol{\mu}_k\Big) 
\end{align*}
which can be further simplied by combining the exponentials to get
\begin{equation*}
M_{\mathbf{X}}(\mathbf{t}) 
= \exp\Big( \sum_{i=1}^{k}\frac{1}{2}c_i^2\mathbf{t}^{\top}\mathsf{\Sigma}_i\,\mathbf{t}  + \sum_{i=1}^{k} c_i\mathbf{t}^{\top}\boldsymbol{\mu}_i \Big) 
= \exp\Big( \frac{1}{2}\mathbf{t}^{\top}\Big(\sum_{i=1}^{k}c_i^2\mathsf{\Sigma}_i\Big)\,\mathbf{t}  + \mathbf{t}^{\top}\Big(\sum_{i=1}^{k} c_i\boldsymbol{\mu}_i \Big)\Big).
\end{equation*}
Now, the uniqueness theorem for MGFs and \cref{eq:MGF:N} imply that the PDF of $\mathbf{X}$ is given by \cref{eq:MVN:LC}
\end{prf}

As an application of the uniqueness theorem consider the following lemma that we shall use later.
\begin{lem}\label{lem:noise}
Let $\mathbf{Y}=(\mathbf{Y}_1, \mathbf{Y}_2)$ be a normal random vector in $\mathbb{R}^{n+m}$ with $p_{\mathbf{Y}}(\mathbf{y})=\mathcal{N}(\mathbf{y}; \boldsymbol{\mu}, \mathsf{\Sigma})$. Let $\mathbf{E}_1$ be a normal random vector in $\mathbb{R}^n$, which is independent of $\mathbf{Y}$ and $p_{\mathbf{E}_1}(\mathbf{y}_1)=\mathcal{N}(\mathbf{y}_1; \mathbf{0}_n, \mathsf{\Gamma}_{11})$. If $\mathbf{Z} = \mathbf{Y} + \mathbf{E}$ with $\mathbf{E}=(\mathbf{E}_1,\mathbf{0}_m)$, then $\mathbf{Z}$ is a normal random vector in $\mathbb{R}^{n+m}$ and its PDF is given by
\begin{equation}
p_{\mathbf{Z}}(\mathbf{y})
= \mathcal{N}(\mathbf{y}; \boldsymbol{\mu}, \mathsf{\Sigma}+\mathsf{\Gamma})
= \mathcal{N} \left(
\begin{bmatrix}
\mathbf{y}_1 \\
\mathbf{y}_2
\end{bmatrix} ;
\begin{bmatrix}
\boldsymbol{\mu}_1 \\
\boldsymbol{\mu}_2
\end{bmatrix} ,
\begin{bmatrix}
\mathsf{\Sigma}_{11}  & \mathsf{\Sigma}_{12} \\
\mathsf{\Sigma}_{21} & \mathsf{\Sigma}_{22}
\end{bmatrix}
+
\begin{bmatrix}
\mathsf{\Gamma}_{11}  & \mathsf{0}_{n,m} \\
\mathsf{0}_{m,n} & \mathsf{0}_{m,m}
\end{bmatrix}
\right) \cdot
\eqnum
\label{eq:PDF:noise}
\end{equation}
\end{lem}
\begin{prf}
Note that $\mathbf{E}$ is not a \textit{continuous} random vector, and so not normal. This means we cannot use \cref{thm:MVN:LC}. However, MGFs come to the rescue again.
\begin{align*}
M_{\mathbf{Z}}(\mathbf{t}) 
&= \mathbb{E}(\exp(\mathbf{t}^{\top}\mathbf{Y} + \mathbf{t}^{\top}\mathbf{E}))
= \mathbb{E}(\exp(\mathbf{t}^{\top}\mathbf{Y})\exp(\mathbf{t}_1^{\top}\mathbf{E}_1))
= \mathbb{E}(\exp(\mathbf{t}^{\top}\mathbf{Y}))\mathbb{E}(\exp(\mathbf{t}_1^{\top}\mathbf{E}_1)) \\ 
&= \exp\Big(\frac{1}{2}\mathbf{t}^{\top}\mathsf{\Sigma}\,\mathbf{t}+\mathbf{t}^{\top}\boldsymbol{\mu}\Big)
\exp\Big(\frac{1}{2}\mathbf{t}_1^{\top}\mathsf{\Gamma}_{11}\,\mathbf{t}_1\Big) 
=  \exp\Big(\frac{1}{2}\mathbf{t}^{\top}(\mathsf{\Sigma} + \mathsf{\Gamma})\,\mathbf{t}+\mathbf{t}^{\top}\boldsymbol{\mu}\Big).
\end{align*}
Employing the uniquness theorem completes the proof.
\end{prf}


\section{Gaussian Process}
 A Gaussian Process (GP) is a special Stochastic Process (SP). Before defining what a GP is, we start by defining an SP.
 \begin{defn}
 A stochastic process is as collection of random variables. This collection can be countable or uncountable. In methematical notation, we write $\{X_{\alpha}|\,\alpha\in \mathcal{X}\}$ with $\mathcal{X}$ being an \textbf{index set} that labels the random variables as $X_{\alpha}$.
 \end{defn}
 \begin{rem}
The choice of the index set is arbitrary. Some choices that usually appear in applications are $\mathcal{X}\subseteq\mathbb{N}$ and $\mathcal{X}\subseteq\mathbb{R}^{m}$ with $m \ge 1$. Stochastic processes are used for modeling states of random evolution of a system with each random variable $X_{\alpha}$ corresponding to a specific random state labeled by $\alpha$.
 \end{rem}
 \begin{exmp}
 A famous example of an SP is the discrete-time Markov chain. It is defined as the set of random variables $\{X_t|\,t\in\mathbb{N}\}$ with the possible values of $X_i$ forming a countable set $S$ called the \textbf{state space} of the chain. These random variables possess the \textbf{Markov property}, namely that the probability of moving to the next state depends only on the present state and not on the previous states, which is formulated as below
 \begin{equation*}
 \mathbb{P}(X_{n+1} = x_{n+1} | X_{n} = x_{n},\dots,X_{1} = x_{1}) =  \mathbb{P}(X_{n+1} = x_{n+1}|X_{n} = x_{n}),
 \end{equation*}
 where you can interpret $t=n$ as the present time. As a concrete example, the number of daily car accidents from an origin in time can be modeled as a Markov chain. Indeed, we don't expect the number of tommorow's car accidents to depend on the number of car accidents on yesterday or one week ago or even today. In other words, the future only depends on the present and not the past.
 \end{exmp}
 
\subsection{What is a Gaussian Process?}
We are now ready to introduce a GP.
\begin{defn}
A Gaussian process is a collection of random variables $\mathscr{C} = \{f(\mathbf{x})|\,\mathbf{x}\in\mathcal{X}\}$, any finite number of which have a joint Gaussian PDF. The process is determined by its \textbf{mean function} $\mu: \mathcal{X} \to \mathbb{R}$ and \textbf{kernel} or \textbf{covariance function} $\kappa: \mathcal{X} \times\mathcal{X} \to \mathbb{R}$ to specify the expected value and covariance of the random variables in the set $\mathscr{C}$ as follows
\begin{alignat*}{2}
\mu(\mathbf{x}) &= \mathbb{E}(f(\mathbf{x})), && \forall \mathbf{x}\in\mathcal{X}, \\
\kappa(\mathbf{x},\mathbf{x}') &= \mathbb{E}\big((f(\mathbf{x})-\mu(\mathbf{x}))(f(\mathbf{x}')-\mu(\mathbf{x}')\big) = \cov(f(\mathbf{x}), f(\mathbf{x}')), \qquad && \forall \mathbf{x},\, \mathbf{x}'\in\mathcal{X}.
\eqnum
\label{eq:GP:def}
\end{alignat*}
Usually, the collection $\mathscr{C}$ is denoted by $\gp(\mu(\mathbf{x}), \kappa(\mathbf{x},\mathbf{x}'))$, so we write
\begin{equation*}
f(\mathbf{x}) \in \gp(\mu(\mathbf{x}), \kappa(\mathbf{x},\mathbf{x}'))
\eqnum
\end{equation*}
to emphasize that $f(\mathbf{x})$ comes from a GP and by definition its PDF will be
\begin{equation*}
p_{f(\mathbf{x})}(t) = \mathcal{N}\big(t; \mu(\mathbf{x}), \kappa(f(\mathbf{x}), f(\mathbf{x}))\big).
\eqnum
\end{equation*}
\end{defn}
\begin{rem}
Suppose that we select $n$ random variables from a $\gp(\mu(\mathbf{x}), \kappa(\mathbf{x},\mathbf{x}'))$ and for \textit{notational convenience} we label them as $Y_i=f(\mathbf{x}_i)$ with $i=1,2,\dots,n$. According to the definition of GP, we know that $p_{\mathbf{Y}}(\mathbf{y}) =\mathcal{N}(\mathbf{y};\boldsymbol{\mu}, \mathsf{\Sigma})$ with
\begin{alignat*}{2}
\mu_i &= \mathbb{E}(Y_i) =\mathbb{E}(f(\mathbf{x}_i)) = \mu(\mathbf{x}_i), && i=1,\dots, n\\
\sigma_{ij} &= \cov(Y_i, Y_j) = \cov(f(\mathbf{x}_i), f(\mathbf{x}_j)) = \kappa(\mathbf{x}_i, \mathbf{x}_j), \qquad && i=1,\dots, n, \qquad j = 1\dots,n,
\eqnum
\label{eq:GP:consis}
\end{alignat*}
where $[\boldsymbol{\mu}]_i=\mu_i$ and $[\mathsf{\Sigma}]_{ij}=\sigma_{ij}$. So the mean vector and the covariance matrix of the joint PDF $\mathcal{N}(\mathbf{y};\boldsymbol{\mu}, \mathsf{\Sigma})$ are determined by the mean function and covariance function of the GP. Now partition the random vector $\mathbf{Y}$ as $(\mathbf{Y}_1, \mathbf{Y}_2)$ with $\mathbf{Y}_1=[\mathbf{Y}]_{1:r}$ and $\mathbf{Y}_2=[\mathbf{Y}]_{r+1:n}$. From the marginalization property of MVNs in \cref{thm:MT:MV} we know that $p_{\mathbf{Y}_1}(\mathbf{y}_1) = \mathcal{N}(\mathbf{y}_1; \boldsymbol{\mu}_1, \mathsf{\Sigma}_{11})$ and $p_{\mathbf{Y}_2}(\mathbf{y}_2) = \mathcal{N}(\mathbf{y}_2; \boldsymbol{\mu}_2, \mathsf{\Sigma}_{22})$, where $\boldsymbol{\mu}_1 = [\boldsymbol{\mu}]_{1:r}$, $\mathsf{\Sigma}_{11}=[\mathsf{\Sigma}]_{1:r,\,1:r}$ and $\boldsymbol{\mu}_2 = [\boldsymbol{\mu}]_{r+1:n}$, $\mathsf{\Sigma}_{22}=[\mathsf{\Sigma}]_{r+1:n,\,r+1:n}$. The natural question to ask is that do we get the same PDFs if we had initially chosen $\mathbf{Y}_1$ or $\mathbf{Y}_2$ from the GP. The answer is YES, since by \cref{eq:GP:consis} we observe that everything remains the same except for the range of indices $i$ and $j$. In other words, examination of a larger set of random variables does not change the distribution of the smaller set. This is well known as the \textbf{consistency requirement}. To recap, a GP is well-defined thanks to the marginalization property of MVNs. 
\end{rem}
\begin{rem}
Note that the consistency requirement would be lost if we had used a precision function to determine the precision matrix of a finite number of selected random variables from the collection $\mathscr{C}$. In that case, the definition is invalid since we could get two different PDFs for the same random vector. Let us repeat the previous experiment. Choose $n$ random variables constructing the random vector $\mathbf{Y}$. The precision matrix $\Lambda$ for $p_{\mathbf{Y}}$ is obtained via the precision function. By the matrix inversion lemma, $\Lambda$ can be inverted to get $\mathsf{\Sigma}_{11} = (\mathsf{\Lambda}_{11} - \mathsf{\Lambda}_{12} \mathsf{\Lambda}_{22}^{-1} \mathsf{\Lambda}_{21})^{-1}$. By the marginalization property of Gaussians, $\Sigma_{11}$ should be the covariance matrix for $p_{\mathbf{Y}_1}$, so its inverse $\Sigma_{11}^{-1}$ will be its precision matrix. If we had chosen $\mathbf{Y}_1$ from the beginning, its precision matrix $\bar{\Lambda}$ would be directly obtained via the percision function. However, the equality $\bar{\Lambda}=\mathsf{\Lambda}_{11} - \mathsf{\Lambda}_{12} \mathsf{\Lambda}_{22}^{-1} \mathsf{\Lambda}_{21}$ does not necessarily hold. In other words, this implies that the examination of a larger set of random variables does influence the distribution of the smaller set.
\end{rem}
\begin{exmp}
If the index set $\mathcal{X}$ is finite, e.g. $\mathcal{X}=\{1,2,\dots,n\}$, then the GP is reduced to an MVN distribution. So GP is a generalization of MVN distribution to infinitely many random variables.
\end{exmp}

\subsection{Probabilistic View of Regression}
A typtical regression problem has the following elements.
\begin{defn}[Training Data]
In the regression problem, we have a training set $\mathcal{D}_1$ of $n$ observations, $\mathcal{D}_1=\{(\mathbf{x}_{1,i}, y_{1,i})|i=1,\dots,n\}$, where $\mathbf{x}_{1,i}\in\mathbb{R}^{d}$ denotes an \text{input} vector and $y_{1,i}\in\mathbb{R}$ denotes the output, target or dependent variable. The inputs are aggregated in the \textbf{design matrix} $\mathsf{X}_1\in \mathbb{R}^{n\times d}$ so its rows are $[\mathsf{X}_1]_{i,1:n}=\mathbf{x}_{1,i}^{\top}$. The targets are collected in the vector $\mathbf{y}_1\in\mathbb{R}^n$. It is also customary to write $\mathcal{D}_1=(\mathsf{X}_1, \mathbf{y}_1)$.
\end{defn}
\begin{defn}[Test Data]
In the regression problem, we have a test set $\mathcal{D}_2=\{(\mathbf{x}_{2,i}, y_{2,i})|i=1,\dots,m\}$, where $\mathbf{x}_{2,i}\in\mathbb{R}^{d}$ denotes a \text{test} point and $y_{2,i}\in\mathbb{R}$ denotes the predicted output by our model at $\mathbf{x}_{2,i}$. The test points are aggregated in $\mathsf{X}_2\in \mathbb{R}^{n\times d}$ so its rows are $[\mathsf{X}_2]_{i,1:n}=\mathbf{x}_{2,i}^{\top}$. The predicted outputs are collected in the vector $\mathbf{y}_2\in\mathbb{R}^n$. It is also customary to write $\mathcal{D}_2=(\mathsf{X}_2, \mathbf{y}_2)$.
\end{defn}
\begin{rem}
Note that the subscripts $1$ and $2$ in the previous definitions refer to the training and test data.
\end{rem}
We shall take a probabilistic approach to solve the regression problem. We assume that the \textbf{observed} targets $\mathbf{y}_1$ and \textbf{unobserved} targets $\mathbf{y}_2$ are samples drawn from a set of random variables belonging to a GP with an index set $\mathcal{X}=\mathbb{R}^d$. The inputs are exactly determined and do not have any source of randomness. Now, the question is that if we look at new input points $\mathbf{x}_2$, what the conditional PDF $p_{\mathbf{Y}_2|\mathbf{Y}_1}(\mathbf{y}_2|\mathbf{y}_1)$ is. More intuitively, we are interested in knowing the odds of seeing the target values $\mathbf{y}_2$ at new input points $\mathsf{X}_2$, given that we have seen target values $\mathbf{y}_1$ at input pionts $\mathsf{X}_1$.  To take a step futher, we should present our \textbf{prior believes} about the GP that generates the target values. Consequently, we should determine the associated mean function and kernel. 
\begin{rem}[Zero Mean Assumption]
For simplicity in our calculations, let us assume that the mean function of the GP is zero, so $\mu(\mathbf{x})=0$ for all $\mathbf{x}\in\mathcal{X}$. Even with this simplifying assumption, we shall see that the model's predictive power is good enough. For a more general mean function formulation consult \cite[Section 2.7]{Rasmussen2006}. The only thing that remains to specify is the kernel function.
\end{rem}
\begin{defn}
The Radial Basis Function (RBF) or the Squared Exponential (SE) kernel is defined as
\begin{equation*}
\kappa(\mathbf{x},\mathbf{x}') = \sigma_y \exp \Big(-\frac{1}{2\ell^2} \lVert\mathbf{x}-\mathbf{x}'\rVert^2\Big),
\eqnum
\label{eq:RBF}
\end{equation*}
where $\sigma_y$ is a \textbf{hyperparameter} representing the amplitude of the covariance and $\ell$ is also a hyperparameter known as the length-scale.
\end{defn}
\begin{rem}
The RBF kernel encodes the intuition that if $\mathbf{x}_i$ and $\mathbf{x}_j$ are close to each other in the input space $\mathcal{X}$, then their correponding target values $y_i$ and $y_j$ are samples drawn from two highly correlated random variables $Y_i=f(\mathbf{x}_i)$ and $Y_j=f(\mathbf{x}_j)$. As the distance between the inputs increases, the correlation between $Y_i$ and $Y_j$ decreases exponentially. Pay attention to the fact that the covariance between \textit{outputs} is written as a function of \textit{inputs}.
\end{rem}
There are a wide range of other kernels that can be used. However, it should be noted that the kernel must have specific properties to guarantee producing SPD covariance matrices. It can be proved that convariance matrices produced by the RBF kernel are SPD. For more details on the kernels refer to \cite[Chapter 4]{Rasmussen2006}.

\subsubsection{Noise-Free Prediction}
An intuitive way to think about GPs is that they are distributions over functions. To see this, think of function values as vectors of infinite length. In detail, we choose \textit{arbitrary} $m$ number of input points $\mathsf{X}_2 \in \mathbb{R}^{m\times d}$ and write out the corresponding covariance matrix $\mathsf{\Sigma_{22}}$ using Eqs. \eqref{eq:GP:consis} and \eqref{eq:RBF} elementwise. Next, we draw a random vector $\mathbf{y}_{2}$ from the following PDF
\begin{equation*}
p_{\mathbf{Y}_{2}}(\mathbf{y}_{2}) = \mathcal{N}(\mathbf{y}_{2}; \mathbf{0}, \mathsf{\Sigma_{22}}).
\eqnum
\label{eq:GP:prior}
\end{equation*}
Indeed, this is a sample drawn from the \textbf{prior distribution} as we have not used our knowledge of the observed targets. The method of sampling from an MVN distribution was described in \cref{exmp:sample}. You can visit \cite{Gortler2019} for a nice visualization of sampling from prior distributions with different kernels and hyperparameters. 

Now, based on our assumption that the target values are samples drawn from random variables that come from a GP, we can write the following
\begin{equation*}
p_{\mathbf{Y}_{1}, \mathbf{Y}_{2}}(\mathbf{y}_{1}, \mathbf{y}_{2}) =
\mathcal{N} \left(
\begin{bmatrix}
\mathbf{y}_1 \\
\mathbf{y}_2
\end{bmatrix} ;
\begin{bmatrix}
\mathbf{0}_n \\
\mathbf{0}_m
\end{bmatrix} ,
\begin{bmatrix}
\mathsf{\Sigma}_{11} & \mathsf{\Sigma}_{12} \\
\mathsf{\Sigma}_{21} & \mathsf{\Sigma}_{22} 
\end{bmatrix}
\right) \cdot
\eqnum
\label{eq:GP:joint}
\end{equation*}
To answer our original question, we use \cref{thm:MVN:MC} to compute $p_{\mathbf{Y}_2|\mathbf{Y}_1}(\mathbf{y}_2|\mathbf{y}_1)$ as below
\begin{equation*}
p_{\mathbf{Y}_2|\mathbf{Y}_1}(\mathbf{y}_2|\mathbf{y}_1) = 
\mathcal{N}(\mathbf{y}_2;\mathsf{\Sigma}_{21}\mathsf{\Sigma}_{11}^{-1}\mathbf{y}_1, \mathsf{\Sigma}_{22} - 
\mathsf{\Sigma}_{21} \mathsf{\Sigma}_{11}^{-1} \mathsf{\Sigma}_{12})
\eqnum
\label{eq:GP:posterior}
\end{equation*}
which is called the \textbf{noise-free} predictive or \textbf{posterior} distribution.  The \textit{prior} and \textit{posterior} terminology come from the well-known \textbf{Bayes rule}
\begin{equation*}
p_{\mathbf{Y}_2|\mathbf{Y}_1}(\mathbf{y}_2|\mathbf{y}_1) =
\frac{p_{\mathbf{Y}_2,\mathbf{Y}_1}(\mathbf{y}_2,\mathbf{y}_1)}{p_{\mathbf{Y}_1}(\mathbf{y}_1)} = 
\frac{p_{\mathbf{Y}_1|\mathbf{Y}_2}(\mathbf{y}_1|\mathbf{y}_2)p_{\mathbf{Y}_2}(\mathbf{y}_2)}{p_{\mathbf{Y}_1}(\mathbf{y}_1)},
\eqnum
\end{equation*}
which is usually seen as
\begin{equation*}
\text{posterior} = \frac{\text{likelihood} \times \text{prior}}{\text{marginal likelihood or evidence}},
\eqnum
\end{equation*}
whose interpretation is that our prior believes combined with our observations (likelihood) construct our posterior predictions. \cref{eq:GP:posterior} is all we need for prediction. According to this equation, the posterior mean is a linear function of the observed targets $\mathbf{y}_1$, while the posterior covariance matrix is constant. Fig. \ref{fig:prior:posterior} summarizes our achievements so far.
\begin{figure}[t!]
\centering
\includegraphics[width=12cm]{figs/prior-posterior.png}
\caption{The left figure shows three random functions drawn from a GP prior. The dots indicate values of $y_{2,i}$ that are actually generated. The right figure depicts three random functions drawn from a GP posterior as lines by joining a large number of evaluated pionts. Notice that the functions look smooth. The + signs are five noise free observation indicated. In both plots the shaded area represents the pointwise mean $\mu_{1|2}$ plus and minus $2\sigma_{1|2}$ known as the 95 percent \textbf{confidence region}. Pay attention to the fact that the variance vanishes rapidly near the observed data points and increases as we get away from them \cite{Rasmussen2006}.}
\label{fig:prior:posterior}
\end{figure}

\subsubsection{Noisy Prediction}\label{sec:Noisy:Pred}
It is typical for more realistic modelling situations that we do not have access to function values themselves, but only noisy versions of them. Consequenlty, it is reasonable to write
\begin{equation*}
\mathbf{Z}_{1} = \mathbf{Y}_{1} + \mathbf{E}_1
\eqnum
\end{equation*}
where $\mathbf{E}_1$ is a random noise vector on the observed data. Assume that $\mathbf{E}_1$ is independent of random vectors $\mathbf{Y}_1, \mathbf{Y}_2$ and so independent of $\mathbf{Y}$. Furthermore, suppose that the noise is normal with the PDF $p_{\mathbf{E}_1}(\boldsymbol{\epsilon})=\mathcal{N}(\boldsymbol{\epsilon}; \mathbf{0}, \sigma_{\epsilon}^2\mathsf{I})$. We can write $(\mathbf{Z}_1, \mathbf{Z}_2) = (\mathbf{Y}_1, \mathbf{Y}_2) + (\mathbf{E}_1, \mathbf{0}_m)$, or more compactly $\mathbf{Z} = \mathbf{Y} + \mathbf{E}$. By \cref{eq:GP:joint}, we know that $\mathbf{Y}$ is a normal random vector.  Using \cref{lem:noise}, we conclude that $\mathbf{Z}$ is also normal and its PDF is given by \cref{eq:PDF:noise}. Considering that $\mathbf{Z}_2=\mathbf{Y_2}$, the PDF of $\mathbf{Z}=(\mathbf{Z}_1, \mathbf{Y}_2)$ is
\begin{equation*}
p_{\mathbf{Z}_{1}, \mathbf{Y}_{2}}(\mathbf{z}_{1}, \mathbf{y}_{2}) =
\mathcal{N} \left(
\begin{bmatrix}
\mathbf{z}_1 \\
\mathbf{y}_2
\end{bmatrix} ;
\begin{bmatrix}
\mathbf{0}_n \\
\mathbf{0}_m
\end{bmatrix} ,
\begin{bmatrix}
\mathsf{\Sigma}_{11}+\sigma_{\epsilon}^2\mathsf{I} & \mathsf{\Sigma}_{12} \\
\mathsf{\Sigma}_{21} & \mathsf{\Sigma}_{22} 
\end{bmatrix}
\right) 
\eqnum
\label{eq:GP:noise:joint}
\end{equation*}
and the predictive distribution becomes
\begin{equation*}
p_{\mathbf{Y}_2|\mathbf{Z}_1}(\mathbf{y}_2|\mathbf{z}_1) =
\mathcal{N}(\mathbf{y}_2; 
\mathsf{\Sigma}_{21}(\mathsf{\Sigma}_{11} + \sigma_{\epsilon}^2\mathsf{I})^{-1}\mathbf{z}_1, 
\mathsf{\Sigma}_{22} - \mathsf{\Sigma}_{21} (\mathsf{\Sigma}_{11}+\sigma_{\epsilon}^2\mathsf{I})^{-1} \mathsf{\Sigma}_{12}).
\eqnum
\label{eq:GP:posterior:noise}
\end{equation*}
In comparison with \cref{eq:GP:posterior}, we have replaced $\mathsf{\Sigma}_{11}$, $\mathbf{Y}_1$,  $\mathbf{y}_1$ with $\mathsf{\Sigma}_{11}+\sigma_{\epsilon}^2\mathsf{I}$,  $\mathbf{Z}_1$, $\mathbf{z}_1$, respectively.

\subsection{Gaussian Process Algorithm}
Let us simplify our formulation of \cref{eq:GP:posterior:noise} a little bit. In the case that there is only one test point we introduce the following notation for the sub-blocks of the covariance matrix
\begin{equation*}
\mathsf{K}:=\mathsf{\Sigma}_{11} ,  \qquad
\mathbf{k}:=\mathsf{\Sigma}_{12}  =\mathsf{\Sigma}_{21}^{\top}, \qquad
k := \mathsf{\Sigma}_{22},
\eqnum
\end{equation*}
and the resulting simplifications follow
\begin{equation*}
\mathbf{Y}_2 = Y_2, \qquad
\mathbf{y}_2 = y_2, \qquad
\boldsymbol{\mu}_{2|1} = \mu_{2|1} \qquad
\mathsf{\Sigma}_{2|1} = \sigma_{2|1}^2.
\eqnum
\end{equation*}
Then, the posterior PDF in \cref{eq:GP:posterior:noise} reduces to a univariate Gaussian
\begin{equation*}
p_{Y_2|\mathbf{Y}_1}(y_2|\mathbf{y}_1) = 
\mathcal{N}(y_2; \mathbf{k}^{\top}(\mathsf{K} + \sigma_{\epsilon}^2\mathsf{I})^{-1}\mathbf{y}_1, k - 
\mathbf{k}^{\top} (\mathsf{K}+\sigma_{\epsilon}^2\mathsf{I})^{-1} \mathbf{k}),
\eqnum
\label{eq:GP:posterior:single}
\end{equation*}
and the posterior mean and variance will be
\begin{align*}
\mu_{2|1}  &= \mathbf{k}^{\top}(\mathsf{K}+\sigma_{\epsilon}^2\mathsf{I})^{-1}\mathbf{y}_1, \\
\sigma_{2|1}^2 &= k - \mathbf{k}^{\top} (\mathsf{K}+\sigma_{\epsilon}^2\mathsf{I})^{-1} \mathbf{k}.
\eqnum
\end{align*}
Noting that $\mathsf{K}+\sigma_{\epsilon}^2\mathsf{I}$ is SPD, using the Cholesky decomposition, we get $\mathsf{K}+\sigma_{\epsilon}^2\mathsf{I}=\mathsf{L}\mathsf{L}^{\top}$. Substituting this into the new formulation we obtain
\begin{align*}
\mu_{2|1}  &= \mathbf{k}^{\top}\mathsf{L}^{-\top}\mathsf{L}^{-1}\mathbf{y}_1, \\
\sigma_{2|1}^2 &= k - (\mathsf{L}^{-1}\mathbf{k})^{\top}(\mathsf{L}^{-1}\mathbf{k}).
\eqnum
\label{eq:GP:posterior:alg}
\end{align*}
\begin{algorithm}[t!]
\caption{Gaussian Process}\label{alg:GP}
\hspace*{\algorithmicindent} {\scshape Input}: $\mathsf{X}_1, \, \mathbf{y}_1, \, \kappa, \sigma_{\epsilon}^2, \, \mathbf{x}_{2,1}$ \\
\hspace*{\algorithmicindent} {\scshape Output}: $\mu_{2|1}, \, \sigma_{2|1}^2$ \\
\hspace*{\algorithmicindent}{\scshape Gaussian-Process}($\mathsf{X}_1, \, \mathbf{y}_1, \, \kappa, \sigma_{\epsilon}^2, \, \mathbf{x}_{2,1}$):
\begin{algorithmic}[1]
\LineComment{Calculate $\mathsf{K}+\sigma_{\epsilon}^2\mathsf{I}$ and its Cholesky decomposition.}
\For{$i=1$ \textbf{to} $n$}
    \For{$j=1$ \textbf{to} $n$}
        \State $[\mathsf{K}]_{ij} = \kappa(\mathbf{x}_{1,i}, \mathbf{x}_{1,j})$
    \EndFor
\EndFor
\State $\mathsf{L}= \text{{\scshape Cholesky-Decomposition}}(\mathsf{K}+\sigma_{\epsilon}^2\mathsf{I})$
\LineComment{Calculate $\mathbf{k}$ and $k$}
\For{$i=1$ \textbf{to} $n$}
    \State $[\mathbf{k}]_{i} = \kappa(\mathbf{x}_{1,i}, \mathbf{x}_{2,1})$
\EndFor
\State $k = \kappa(\mathbf{x}_{2,1}, \mathbf{x}_{2,1})$
\LineComment{Calculate $\mu_{2|1}$.}
\State Solve $\mathsf{L} \mathbf{b} = \mathbf{y}_1$
\State Solve $\mathsf{L}^{\top} \mathbf{a} = \mathbf{b}$
\State $\mu_{2|1} = \mathbf{k}^{\top}\mathbf{a}$
\LineComment{Calculate $\sigma_{2|1}^2$.}
\State Solve $\mathsf{L} \mathbf{c} = \mathbf{k}$
\State $\sigma_{2|1}^2=k-\mathbf{c}^{\top}\mathbf{c}$
\State \Return ($\mu_{2|1}$, $\sigma_{2|1}^2$)
\end{algorithmic}
\end{algorithm}
Using \cref{eq:GP:posterior:alg}, Algorithm \ref{alg:GP} addresses a simple method for calculating the predictive mean, variance, and hence the PDF in \cref{eq:GP:posterior:single}.  It should be noted that we avoid computing the inverse directly as it is considered problematic from a numerical perpective \cite[Section 4.7]{Gill1991}. The main work is to compute the Cholesky decomposition, which takes $\mathcal{O}(\frac{1}{6}n^3)$ time. Solving the upper and lower-triangular linear systems in lines 14 and 15 takes $\mathcal{O}(\frac{1}{2}n^2)$ time. The dot produtcs in lines 16 and 19  and the matrix and vector sums in lines 7 and 19 requires $\mathcal{O}(n)$ time. Assuming that we use an SE kernel (RBF), calculating $\mathsf{K}$ takes a work of $\mathcal{O}(n^2d)$ and calculating $\mathbf{k}$ requires $\mathcal{O}(nd)$. Summing all these up, the running time $T(n,d)$ of the GP algorithm is of order
\begin{equation*}
T(n,d) \in \mathcal{O}\Big(\frac{1}{6}n^3+n^2d+\frac{3}{2}n^2+nd+n\Big),
\eqnum
\end{equation*}
where $n$ is the size of our dataset and $d$ is the number of features or dimension of the input space. In the case of $n \gg d$, this simplifies to
\begin{equation*}
T(n,d) \in \mathcal{O}\Big(\frac{1}{6}n^3\Big),
\eqnum
\end{equation*}
showing a cubic time complexity in dataset size for the GP algorithm.
\begin{rem}
In order to make predication at different test points $\mathbf{x}_{2, l}$, you need to run \cref{alg:GP} for $l=1,\dots,m$, where $m$ is the number of test points. In order to make this more efficient, pay attention that the lines 2-7 and 14-15 does not depend on $\mathbf{x}_{2, l}$ so you can do their calculations once and employ it at every iteration.
\end{rem}
\subsection{Kernels and Hyperparameters}
Typically, the kernel functions that we use will have some free parameters, known as hyperparameters that are not determined during the learning process. According to Eqs. \eqref{eq:RBF} and \eqref{eq:GP:posterior:single}, the hyperparameters for a noisy GP prediction with the SE kernel are the length-scale $\ell$, the signal variance $\sigma_y$, and the noise variance $\sigma_{\epsilon}$.  The hyperparameter $\sigma_y$ controls the amplitude of the covariance between the random variables $f(\mathbf{x})$ and $f(\mathbf{x}')$. $\sigma_{\epsilon}$ determines the amplitude of the variance of the noise $\mathcal{E}_i$. The hyperparameter $\ell$ determines a neighborhood radius around an input point $\mathbf{x}$ such that for every other input point $\mathbf{x}'$ lying in that neighborhood the corresponding random variables $f(\mathbf{x})$ and $f(\mathbf{x}')$ have a high correlation. Fig. \ref{fig:length:scale} illustrates the effect of this hyperparameter on the predictive distribution. The overall message of the figure is that the hyperparameters can significantly affect the posterior distribution. This gives rise to the question of how should we choose these hyperparameters. In the following, we address two methods for tunning hyperparameters, but this can be studied in more depth by consulting the Bible of GPs \cite[Chapter 5]{Rasmussen2006}.
\begin{figure}[t!]
\centering
\includegraphics[width=17cm]{figs/length-scale.png}
\caption{In all three figures + signs show the training data and the solid blue line is the posterior mean $\mu_{2|1}$. The figures show GPs with SE kernels and different hyperparameters fit to 20 noisy observations. In the left figure, the hyperparameters are $(\ell, \sigma_y, \sigma_{\epsilon})=(0.3, 1.08, 0.00005)$ that corresponds to a short length-scale and as we see the wiggly looking mean oscillates so rapidly that it has fit almost all training data, ringing the bell for possible overfitting . In the middle figure, the hyperparameter are $(\ell, \sigma_y, \sigma_{\epsilon})=(1, 1, 0.1)$ that corresponds to a medium length-scale. The oscillations of the mean are reduced in comparison with the left figure. This is in agreement with our intuition that the length-scale represents a neighborhood radius that you can move in the input space before the target values change drastically. In the right figure, the hyper parameters are  $(\ell, \sigma_y, \sigma_{\epsilon})=(3, 1.16, 0.89)$, which corresponds to a long length-scale. As we see, the mean varies so slowly and has not fit many of the training data, showing that the model's bias has increased significantly. In all of the figures, if the $x$-axis were extended one would observe that the width of the confidence region $4\sigma_{2|1}$ remains constant and reaches the value $4\sigma_y$. You can get more sense about this phenomenon by looking at \cref{eq:GP:posterior:alg} \cite{Rasmussen2006}.}
\label{fig:length:scale}
\end{figure}

\subsubsection{Cross Validation}
In this sub-section, we consider how to use methods of cross-validation (CV) for determining hyperparameters. The basic idea is to split the training set into two disjoint sets, one which is actually used for training, and the other, the \textbf{validation set}, which is used to monitor performance, as shown in \cref{fig:cross:validation}. The performance on the validation set is used as a measure for the \textbf{generalization error} and determining hyperparameters is done based on this.
\begin{figure}[b!]
\centering
\includegraphics[width=9cm]{figs/cross-validation.png}
\caption{The figure illustrates a $5$-fold cross validation \cite{sklearn}.}
\label{fig:cross:validation}
\end{figure}
In practice, a drawback of this method is that only a fraction of the full data set can be used for training, and that if the validation set is small, the performance estimate obtained may have large variance. To minimize these problems, CV is almost always used in the $k$-\textbf{fold} setting. In $k$-fold CV, data is split into $k$ disjoint equally sized subsets. Validation is done on a single subset and training is done using the union of the remaining $k -1$ subsets, the entire procedure being repeated $k$ times, each time with a different subset for validation. Thus, a large fraction of the data can be used for training, and all cases appear as validation cases. The price is that $k$ models must be trained instead of one. Typical values for $k$ are in the range 3 to 10. An extreme case of $k$-fold cross-validation is obtained for $k = n$, the number of training data points, also known as \textbf{Leave-One-Out Cross-Validation} (LOO-CV). Often the computational cost of LOO-CV (training $n$ models) is high. Cross-validation can be used with any loss function. Although the \textbf{squared error loss} is by far the most common for regression \cite[Chapter 5]{Rasmussen2006}. 

To summarize, you should consider discrete values for each hyperparameter and construct a \textbf{search grid} for each split shown in \cref{fig:cross:validation}. Then, you calculate the squared error loss for each point of the grid in a fixed split. Repeating this for all of the splits, you obtain $k$ different squared error loss for each point of the grid. Finally, you can choose that value of the hyperparameters that results in the minimum averaged squared error loss over all the $k$ values.

\subsubsection{Marginal Likelihood}
\begin{figure}[t!]
\centering
\includegraphics[width=17cm]{figs/marginal-likelihood.png}
\caption{The left figure shows the marginal likelihood as a function of the hyperparameters $\ell$ and $\sigma_{\epsilon}$. Signal's standard
deviation is chosen to be $\sigma_y^2 = 1$ for a dataset of 7 observations shown by + signs in the other two figures. There are two local optima, indicated with + signs in the left figure. The middle figure shows the function corresponding to the lower left local maximum that has the hyperparameters $(\ell, \sigma_{\epsilon}^2)=(1, 0.2)$ and is indeed the global maximum. This is quite \textit{wiggly} and has low noise and short length-scale. The function corresponding to the top right local maximum has the hyperparameters $(\ell, \sigma_{\epsilon}^2)=(10, 0.8)$. This is quite smooth and has high noise and long length-scale \cite[Section 5.4.1]{Rasmussen2006}. With only 7 data points, there is not enough evidence to confidently decide which is more reasonable, although the more oscillating model (middle figure) has a marginal likelihood that is about 60 percent higher than the other model (right figure)\cite[Section 15.2.4]{Murphy2012}.}
\label{fig:marginal:likelihood}
\end{figure}
In this method, we look for those hyperparameters that maximize the probability of seeing the training data, i.e. maximizing the marginal likelihood (ML) $p_{\mathbf{Z}_1}(\mathbf{z}_1)$. This is indeed based on the \textbf{frequentist} view of statistics. From \cref{eq:GP:noise:joint}, we know that
\begin{equation*}
p_{\mathbf{Z}_1}(\mathbf{z}_1) = 
\mathcal{N}(\mathbf{z}_1; \mathbf{0}, \mathsf{\Sigma}+\sigma_{\epsilon}\mathsf{I}) = 
\mathcal{N}(\mathbf{z}_1; \mathbf{0}, \mathsf{K}_z),
\eqnum
\end{equation*}
where we have used the definition $\mathsf{K}_z:= \mathsf{K}+\sigma_{\epsilon}^2\mathsf{I}$. If we denote the vector of hyperparameters in a GP model by $\boldsymbol{\theta}\in\mathbb{R}^h$, then the ML method looks for the following maximizer
\begin{equation}
\boldsymbol{\theta^{*}} = \argmax_{\boldsymbol{\theta}\in\mathbb{R}^h} p_{\mathbf{Z}_1}(\mathbf{z}_1; \boldsymbol{\theta}) =   
\argmax_{\boldsymbol{\theta}\in\mathbb{R}^h} \log p_{\mathbf{Z}_1}(\mathbf{z}_1; \boldsymbol{\theta}),
\eqnum
\label{eq:marginal}
\end{equation}
where the second equality follows because the natural logrithm is an strictly increasing function and does not change the optimizer. The term \textit{marginal} refer to the fact that the likelihood $p_{\mathbf{Z}_1}(\mathbf{z}_1;\theta)$ can be obtained by perfoming marginalization on the PDF given in \cref{eq:GP:noise:joint}. This is sometimes refered as \textbf{emprical Bayes} since pure Baysians believe that this method is fallacious. They argue that when we solve the optimization problem in \cref{eq:marginal} then the hyperparameters $\boldsymbol{\theta}$ will depend on the seen data $\mathbf{y}_1$; however, all of our calculation was based on the fact that they are independent of the observed data \cite[Lecture 9]{Freitas2013}. For this reason, more complicated methods such as Monte Carlo (MC) methods have been developed to optimize the hyperparametrs \cite[Section 45.5]{MacKay2003}. With this in mind, ML is usually done in practice and the numerical results are satisfactory. 

To solve the nonlineaer unconstrained optimization problem in \cref{eq:marginal}, one needs to calculate the gradient. Feeding the result into any gradient solver for such problems could find the solution. According to \cref{eq:GP:noise:joint}, natural logarithm of the ML (log-ML) will be
\begin{equation*}
\log p_{\mathbf{Z}_1}(\mathbf{z}_1;\theta) = 
\log\mathcal{N}(\mathbf{z}_1;\mathbf{0}, \mathsf{K}_z) =
-\frac{1}{2}\mathbf{z}_1^{\top}\mathsf{K}_z^{-1}\mathbf{z}_1-\frac{1}{2}\log\det\mathsf{K}_z-\frac{n}{2}\log2\pi,
\eqnum
\label{eq:ML:log}
\end{equation*}
To calculate the gradient, we need the following lemma
\begin{lem}
Suppose that $\mathsf{K}(\boldsymbol{\theta})\in\mathbb{R}^{n,n}$ is an invertible matrix whose elements depend on the vector $\boldsymbol{\theta}\in\mathbb{R}^h$. The the following matrix derivative identities are valid
\begin{equation*}
\partial_{\theta_i} \mathsf{K}^{-1} = - \mathsf{K}^{-1}\partial_{\theta_i}\mathsf{K}\,\,\mathsf{K}^{-1}, \qquad
\partial_{\theta_i}\log\det\mathsf{K} = \tr(\mathsf{K}^{-1}\partial_{\theta_i}\mathsf{K}), \qquad
i=1,\dots,h,
\eqnum
\label{eq:matrix:der}
\end{equation*}
where $\partial_{\theta_i} \mathsf{K}$ is an elementwize derivative with respect to $\theta_i$ and $\tr$ is the trace of a matrix.
\end{lem}
Using \cref{eq:matrix:der}, computing the gradient of log-ML in \cref{eq:ML:log} leads to
\begin{align*}
\partial_{\theta_i}\log p_{\mathbf{Z}_1}(\mathbf{z}_1;\theta) &= 
\frac{1}{2}\mathbf{z}_1^{\top}\mathsf{K}_z^{-1}\partial_{\theta_i}\mathsf{K}_z\,\,\mathsf{K}_z^{-1}\mathbf{z}_1 - 
\frac{1}{2} \tr(\mathsf{K}_z^{-1}\partial_{\theta_i}\mathsf{K}_z) \\
&= \frac{1}{2}\mathbf{a}^{\top}\partial_{\theta_i}\mathsf{K}_z\,\,\mathbf{a} - \frac{1}{2} \tr(\mathsf{K}_z^{-1}\partial_{\theta_i}\mathsf{K}_z) \\
&= \frac{1}{2}\tr(\mathbf{a}^{\top}\partial_{\theta_i}\mathsf{K}_z\,\,\mathbf{a}) - \frac{1}{2} \tr(\mathsf{K}_z^{-1}\partial_{\theta_i}\mathsf{K}_z) \\
&= \frac{1}{2}\tr(\mathbf{a}\mathbf{a}^{\top}\partial_{\theta_i}\mathsf{K}_z) - \frac{1}{2} \tr(\mathsf{K}_z^{-1}\partial_{\theta_i}\mathsf{K}_z) \\
&= \frac{1}{2}\tr((\mathbf{a}\mathbf{a}^{\top}-\mathsf{K}_z^{-1})\partial_{\theta_i}\mathsf{K}_z), \\
&= \frac{1}{2}\tr\Big(\big((\mathsf{K}_z^{-1}\mathbf{z}_1)(\mathsf{K}_z^{-1}\mathbf{z}_1)^{\top}-\mathsf{K}_z^{-1}\big)\partial_{\theta_i}\mathsf{K}_z\Big),
\eqnum
\label{eq:ML:log:grad}
\end{align*}
where we used the fact that the trace of a scalar equals the scalar itself along with the linearity of the trace and the general identity $\tr(\mathsf{A}\mathsf{B})=\tr(\mathsf{B}\mathsf{A})$. The computational cost for the ML in \cref{eq:ML:log:grad} is dominated by computing the inverse $\mathsf{K}_z^{-1}$. More specifically, since $\mathsf{K}_z$ is SPD, its inverse can be computed by first calculating the Cholesky decomposition and solving $n$ resulting upper- and lower-triangular linear systems, which takes a work of $\mathcal{O}(n^3)$.

Fig. \ref{fig:marginal:likelihood} illustrates an example of finiding the optimal values for hyperparameters $\boldsymbol{\theta}=(\ell, \sigma_{\epsilon})$ of a GP model with an SE kernel. We observe that there may exist several local optimums as the marginal likelihood is not a convex function.

\subsubsection{Constructing New Kernels}
As described earlier, the power of GP lies in the choice of the kernel function. This property allows us to introduce domain knowledge into the process and lends GPs their flexibility to capture trends in the training data. For example, by choosing a suitable length-scale for the RBF kernel, we can control how smooth and oscillating the resulting function will be. A big benefit that kernels provide is that they can be combined together, resulting in a more specialized kernel. The decision of which kernel to use is highly dependent on prior knowledge about the data, e.g. if certain characteristics are expected \cite{Gortler2019}.

The only constraint on our choice of covariance function is that it must generate a SPD covariance matrix for any set of input points. In this connection, we have following theorem
\begin{thm}
The sum and product of two kernels is a kernel.
\end{thm}
\begin{figure}[t!]
\centering
\includegraphics[width=17cm]{figs/combine-kernels.png}
\caption{The figures show samples drawn from prior distributions with different kernels. Addition results in an oscillating function with a global trend, while multiplication increases the oscillation amplitude outwards \cite{Gortler2019}.}
\label{fig:kernels:combine}
\end{figure}
For the proof of this theorem, a wide range of kernel functions, and other possibilities of combining kernels see \cite[Section 4.2]{Rasmussen2006}. For example, the sum property can be used to add RBF kernels with different characteristic length-scales and product rule can be employed to produce trending functions with varying oscillations. Fig. \ref{fig:kernels:combine} depicts combinations of a linear and a periodic kernel via summation and multiplication. The two aforementioned kernels are defined as
\begin{align*}
\kappa_{\text{per}}(x,x') &= \sigma_y\exp\Big(-\frac{2}{\ell^2}\sin^2\Big(\frac{\pi( x-x')}{p}\Big)\Big), \\
\kappa_{\text{lin}}(x, x') &= \sigma_b+\sigma_y(x-c)^{\top}(x'-c),
\eqnum
\end{align*}
where $(\ell, \sigma_y, p)$ are the hyperparameters of the periodic kernel and $(\sigma_b, \sigma_y, c)$ are hyperparameters of the linear kernel. For more general forms of these kernel functions in higher dimensional input spaces see \cite[Section 45.4]{MacKay2003}.

\subsection{A Real World Example}
Here, we mention an interesting real world example of GPs in action. The computer graphics and animation fields are filled with applications that require the setting of tricky parameters. In many cases, the models are complex and the parameters are unintuitive for non-experts. \citeauthor{Brochu2010} \cite{Brochu2010} present an optimization method for setting parameters of a procedural fluid animation system by showing the user examples of different parametrized animations and asking for feedback. They simulate the fluid flow by taking the curl of a vector valued potential function which has several technical parameters. In this example, the the aforementioned parameters are the inputs $\mathbf{x}$ and the targets $\mathbf{y}$ are the feedbacks of the user and some previous default data. By introducing an \textbf{active learning} method, they employ the Bayesian technique of bringing in prior belief based on previous runs of the system and expert knowledge to assist users find good parameter settings in as few steps as possible. At the heart of the machinary they have produced, lies the GP introduced in this tutorial. The main benefit is that the GP helps construct an efficient \textbf{explore and exploit} strategy in the input space based on the distribution of uncertainty in that space \cite[Lecture 9]{Freitas2013}. Watch the video of the \textbf{interactive interface} they have created to help users create animations without taking the burdon of adjusting parameters manually \cite{Brochu2010Animation}. Indeed, this idea is not restericted to computer graphics field and can be employed anywhere else that requires the adjusment of techincal parameters.


\section{What is Next?}
In this tutorial, we took a trip from the basics of GPs to their real world applications. We learned that GPs are \textbf{supervised probabilistic} models with high capability for doing regression. Fortunately, they can also be extended for classification tasks. For this purpose, the main idea is to pass the output values  through a sigmoid function. You can study more about this in \cite[Chapter 3]{Rasmussen2006}. More surprisingly, GPs have several models as their special cases. Indeed, some of the well known results are that
\begin{enumerate}[noitemsep]
\item Using \cref{eq:GP:posterior:alg} and Mercer's theorem \cite[Section 4.3]{Rasmussen2006}, it can be shown that the posterior mean $\mu_{1|2}$ is equivalent to a Regression model with nonlinear basis functions and \textbf{inifinite} number of parameters. In this regard, it is said that a GP model is a \textbf{non-parametric} model. Despite what the name suggests, it is meant that it can have as many parameters as required.
\item The Ridge regression model can be recoverd by the posterior mean $\mu_{2|1}$, if we choose a dot product kernel $\kappa(\mathbf{x},\mathbf{x}')=\mathbf{x}^{\top}\mathbf{x}'$ and set the noise variance $\sigma_{\epsilon}$ equal to the regularization parameter $\delta$ \cite[Lecture 9]{Freitas2013}.
\item It can also be shown that specific large neural networks are equivalent to a GP \cite{MacKay1998}.
\end{enumerate}
The main drawback of GPs is their $\mathcal{O}(n^3)$ time complexity, which makes it hard to scale. One of the main trends of research in recent years is to make GPs appropriate for large datasets. You can start reading about this in \cite[Chapter 8]{Rasmussen2006}. A good review about scalable GPs is also presented in \cite{Liu2020}.


\section{Support}
I am happy that you have read this so far, showing that you are serious about understanding GPs and machine learning. I tried to write an easy-to-follow and at the same time rigorous introduction to GPs and I hope that it appealed to you. If you liked this tutorial, please give the \href{https://github.com/Hosein-Rahnama/Gaussian-Processes.git}{GitHub repository} of this document a star. Futhermore, you are more than welcomed to report any errors by opening an issue or to suggest further improvements by making pull requests on the repository.


\section{References}
\printbibliography[heading=none]

\end{document}